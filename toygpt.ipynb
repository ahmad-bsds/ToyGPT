{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"toc_visible":true},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# GPT from Scratch","metadata":{"id":"KBMRpDp64Enq"}},{"cell_type":"markdown","source":"## --------------- Attension -------------------","metadata":{"id":"9zSeq4VokfqY"}},{"cell_type":"markdown","source":"## Get dataset","metadata":{"id":"iNHA4fwHFQpR"}},{"cell_type":"code","source":"!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hl_8PKeWCw_L","outputId":"35e0d012-ab32-4e48-d0e3-94b2f11fe9bc","trusted":true,"execution":{"iopub.status.busy":"2025-07-16T12:40:05.385774Z","iopub.execute_input":"2025-07-16T12:40:05.386398Z","iopub.status.idle":"2025-07-16T12:40:05.719090Z","shell.execute_reply.started":"2025-07-16T12:40:05.386370Z","shell.execute_reply":"2025-07-16T12:40:05.718409Z"}},"outputs":[{"name":"stdout","text":"--2025-07-16 12:40:05--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1115394 (1.1M) [text/plain]\nSaving to: ‘input.txt’\n\ninput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.03s   \n\n2025-07-16 12:40:05 (34.6 MB/s) - ‘input.txt’ saved [1115394/1115394]\n\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"with open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()","metadata":{"id":"Qojhv3LhDBfM","trusted":true,"execution":{"iopub.status.busy":"2025-07-16T09:06:40.883145Z","iopub.execute_input":"2025-07-16T09:06:40.883901Z","iopub.status.idle":"2025-07-16T09:06:40.890151Z","shell.execute_reply.started":"2025-07-16T09:06:40.883846Z","shell.execute_reply":"2025-07-16T09:06:40.889439Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"## Explore","metadata":{"id":"HM_GwImiFVQs"}},{"cell_type":"code","source":"print(f\"{len(text)/1000000} M Charecters.\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fck-W2QZDBht","outputId":"f1dc54c1-49a3-459b-8b41-e508d7225a21","trusted":true,"execution":{"iopub.status.busy":"2025-07-16T09:03:09.287106Z","iopub.execute_input":"2025-07-16T09:03:09.287853Z","iopub.status.idle":"2025-07-16T09:03:09.292556Z","shell.execute_reply.started":"2025-07-16T09:03:09.287807Z","shell.execute_reply":"2025-07-16T09:03:09.291679Z"}},"outputs":[{"name":"stdout","text":"1.115394 M Charecters.\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"print(text[:1000])","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FntXa1m4DBkk","outputId":"f0b22c4d-cf43-4268-b20c-447e73e8a25a","trusted":true,"execution":{"iopub.status.busy":"2025-07-16T09:03:14.292142Z","iopub.execute_input":"2025-07-16T09:03:14.292462Z","iopub.status.idle":"2025-07-16T09:03:14.297682Z","shell.execute_reply.started":"2025-07-16T09:03:14.292438Z","shell.execute_reply":"2025-07-16T09:03:14.296658Z"}},"outputs":[{"name":"stdout","text":"First Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou are all resolved rather to die than to famish?\n\nAll:\nResolved. resolved.\n\nFirst Citizen:\nFirst, you know Caius Marcius is chief enemy to the people.\n\nAll:\nWe know't, we know't.\n\nFirst Citizen:\nLet us kill him, and we'll have corn at our own price.\nIs't a verdict?\n\nAll:\nNo more talking on't; let it be done: away, away!\n\nSecond Citizen:\nOne word, good citizens.\n\nFirst Citizen:\nWe are accounted poor citizens, the patricians good.\nWhat authority surfeits on would relieve us: if they\nwould yield us but the superfluity, while it were\nwholesome, we might guess they relieved us humanely;\nbut they think we are too dear: the leanness that\nafflicts us, the object of our misery, is as an\ninventory to particularise their abundance; our\nsufferance is a gain to them Let us revenge this with\nour pikes, ere we become rakes: for the gods know I\nspeak this in hunger for bread, not in thirst for revenge.\n\n\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"## Preprocess","metadata":{"id":"h27UzsTzFYoD"}},{"cell_type":"markdown","source":"### Unique Charecters","metadata":{"id":"FfKFk2AoF0cV"}},{"cell_type":"code","source":"# Unique charecters available in the dataset.\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\nprint(''.join(chars))\nprint(vocab_size)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g3eObDrcDBnh","outputId":"d6b33baa-7635-4dda-8c79-a312509b400b","trusted":true,"execution":{"iopub.status.busy":"2025-07-16T09:06:45.804555Z","iopub.execute_input":"2025-07-16T09:06:45.804818Z","iopub.status.idle":"2025-07-16T09:06:45.820550Z","shell.execute_reply.started":"2025-07-16T09:06:45.804795Z","shell.execute_reply":"2025-07-16T09:06:45.820007Z"}},"outputs":[{"name":"stdout","text":"\n !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n65\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"There are 56 types of charecters used in this data.","metadata":{"id":"adDFDI0tFi9m"}},{"cell_type":"markdown","source":"### Tokenizer","metadata":{"id":"wGYY8wPWF32C"}},{"cell_type":"code","source":"# tokenize\nstr_to_int = { ch:i for i,ch in enumerate(chars) }\nint_to_string = { i:ch for i,ch in enumerate(chars) }\n\n# Encode\nencode = lambda s: [str_to_int[c] for c in s]\n# Decode\ndecode = lambda l: ''.join([int_to_string[i] for i in l])\n\nprint(encode(\"hii there\"))\nprint(decode(encode(\"hii there\")))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BjlpKT39DBqP","outputId":"3f6d55c2-2c01-45b1-c83f-b2c0e6730d4c","trusted":true,"execution":{"iopub.status.busy":"2025-07-16T09:06:48.549104Z","iopub.execute_input":"2025-07-16T09:06:48.549361Z","iopub.status.idle":"2025-07-16T09:06:48.554738Z","shell.execute_reply.started":"2025-07-16T09:06:48.549339Z","shell.execute_reply":"2025-07-16T09:06:48.554061Z"}},"outputs":[{"name":"stdout","text":"[46, 47, 47, 1, 58, 46, 43, 56, 43]\nhii there\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"Tokenization\n\n- 46 -> h\n- 47 -> i\n- 47 -> i\n\n-----------------\n\n- 1 -> <space>\n- 58 -> t\n- 46 -> h\n- 43 -> e\n- 56 -> r\n- 43 -> e","metadata":{"id":"o0Db2r70Ht-I"}},{"cell_type":"markdown","source":"### Tokenizing entire dataset","metadata":{"id":"px46kc9PIsbo"}},{"cell_type":"code","source":"import torch\n# Tokeize and store in tensor\ndata = torch.tensor(encode(text), dtype=torch.long)\nprint(data.shape, data.dtype)\nprint(data[:1000]) # First 1000 which are charecters encoded.","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tI-EJMStDBtP","outputId":"cbe9a44e-dfa4-4ad8-cb7c-d00f1974b0d0","trusted":true,"execution":{"iopub.status.busy":"2025-07-16T09:06:50.678256Z","iopub.execute_input":"2025-07-16T09:06:50.678939Z","iopub.status.idle":"2025-07-16T09:06:54.921956Z","shell.execute_reply.started":"2025-07-16T09:06:50.678912Z","shell.execute_reply":"2025-07-16T09:06:54.921199Z"}},"outputs":[{"name":"stdout","text":"torch.Size([1115394]) torch.int64\ntensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"### Split for training","metadata":{"id":"Zh4tHwWSJaEc"}},{"cell_type":"code","source":"# Split data into train and validation.\nn = int(0.9*len(data))\ntrain_data = data[:n]\nval_data = data[n:]","metadata":{"id":"khdDQeDMDBwm","trusted":true,"execution":{"iopub.status.busy":"2025-07-16T09:06:54.923063Z","iopub.execute_input":"2025-07-16T09:06:54.923369Z","iopub.status.idle":"2025-07-16T09:06:54.927113Z","shell.execute_reply.started":"2025-07-16T09:06:54.923351Z","shell.execute_reply":"2025-07-16T09:06:54.926274Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"### Chunking","metadata":{"id":"u-rRXCMqJdH5"}},{"cell_type":"markdown","source":"Training model on whole dataset at once is computationally expensive, so what we do is to chunk the data. Model takes each chunk to train on it.","metadata":{"id":"BH-H8vd8J2WL"}},{"cell_type":"code","source":"# Chunk size, let say.\nblock_size = 8","metadata":{"id":"9OAJ03oCDBzq","trusted":true,"execution":{"iopub.status.busy":"2025-07-16T09:06:55.350324Z","iopub.execute_input":"2025-07-16T09:06:55.350833Z","iopub.status.idle":"2025-07-16T09:06:55.354067Z","shell.execute_reply.started":"2025-07-16T09:06:55.350806Z","shell.execute_reply":"2025-07-16T09:06:55.353392Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# First chunk in our data is\n\ntrain_data[:block_size+1]","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-_ZX5mOfDB23","outputId":"4519ed2a-14c8-4053-ed05-79417c53fe01","trusted":true,"execution":{"iopub.status.busy":"2025-07-16T09:06:56.992694Z","iopub.execute_input":"2025-07-16T09:06:56.993084Z","iopub.status.idle":"2025-07-16T09:06:56.999603Z","shell.execute_reply.started":"2025-07-16T09:06:56.993057Z","shell.execute_reply":"2025-07-16T09:06:56.998908Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"So, in out language modeling let say we have *hi ther*, in this context *e* will come next so it will become \"hi there\". That is how charecter level prediction is done.\n\nIn previous example block size:\nIf we have 18 then 47 will come next, 18, 47, then 56 will come next, 18, 47, 56 then 57 will come next, and so on.","metadata":{"id":"ksZ6GFcMLFBg"}},{"cell_type":"code","source":"# Here is an example how next token is preidcted.\nx = train_data[:block_size]\ny = train_data[1:block_size+1]\nfor t in range(block_size):\n    context = x[:t+1]\n    target = y[t]\n    print(f\"when input is {context} the target prediction is: {target}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sXdR1QZQDB6B","outputId":"7c1a892f-d117-4eca-b891-724745aeb4c3","trusted":true,"execution":{"iopub.status.busy":"2025-07-16T09:06:59.263487Z","iopub.execute_input":"2025-07-16T09:06:59.263786Z","iopub.status.idle":"2025-07-16T09:06:59.272021Z","shell.execute_reply.started":"2025-07-16T09:06:59.263765Z","shell.execute_reply":"2025-07-16T09:06:59.271262Z"}},"outputs":[{"name":"stdout","text":"when input is tensor([18]) the target prediction is: 47\nwhen input is tensor([18, 47]) the target prediction is: 56\nwhen input is tensor([18, 47, 56]) the target prediction is: 57\nwhen input is tensor([18, 47, 56, 57]) the target prediction is: 58\nwhen input is tensor([18, 47, 56, 57, 58]) the target prediction is: 1\nwhen input is tensor([18, 47, 56, 57, 58,  1]) the target prediction is: 15\nwhen input is tensor([18, 47, 56, 57, 58,  1, 15]) the target prediction is: 47\nwhen input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target prediction is: 58\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"### Batch","metadata":{"id":"k5dLTFprM3uW"}},{"cell_type":"markdown","source":"So when we will give chunks to the transformer, many chunks will be stacked and give to the transformer. The number of stacked chunks are called **Batch size**. It makes parallel processing.","metadata":{"id":"lMyqX7SaMyBe"}},{"cell_type":"code","source":"batch_size = 4 #independent sequences will be processed in parallel.\nblock_size = 8 # context length for prediction.\n\ntorch.manual_seed(1337)\n\ndef get_batch(split):\n  \"\"\"Generate batch of data input x and target y.\"\"\"\n  data = train_data if split == 'train' else val_data\n  ix = torch.randint(len(data) - block_size, (batch_size,))\n  x = torch.stack([data[i:i+block_size] for i in ix])\n  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n  return x, y\n\nx_batch, y_batch = get_batch('train') # input, target\nprint('inputs:')\nprint(x_batch.shape, \"(batch size, chunk size)\")\nprint(x_batch)\nprint('targets:')\nprint(y_batch.shape, \"(batch size, chunk size)\")\nprint(y_batch)\n\nprint('----')\n\n\nfor b in range(batch_size): # batch dimension\n    for t in range(block_size): # time dimension\n        context = x_batch[b, :t+1]\n        target = y_batch[b,t]\n        print(f\"when input is {context.tolist()} the target prediction is: {target}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kgpy0qzCDB9B","outputId":"2c7766eb-d79b-49bb-8f55-487e9f6a87c4","trusted":true,"execution":{"iopub.status.busy":"2025-07-16T09:07:02.454148Z","iopub.execute_input":"2025-07-16T09:07:02.454606Z","iopub.status.idle":"2025-07-16T09:07:02.491136Z","shell.execute_reply.started":"2025-07-16T09:07:02.454583Z","shell.execute_reply":"2025-07-16T09:07:02.490339Z"}},"outputs":[{"name":"stdout","text":"inputs:\ntorch.Size([4, 8]) (batch size, chunk size)\ntensor([[24, 43, 58,  5, 57,  1, 46, 43],\n        [44, 53, 56,  1, 58, 46, 39, 58],\n        [52, 58,  1, 58, 46, 39, 58,  1],\n        [25, 17, 27, 10,  0, 21,  1, 54]])\ntargets:\ntorch.Size([4, 8]) (batch size, chunk size)\ntensor([[43, 58,  5, 57,  1, 46, 43, 39],\n        [53, 56,  1, 58, 46, 39, 58,  1],\n        [58,  1, 58, 46, 39, 58,  1, 46],\n        [17, 27, 10,  0, 21,  1, 54, 39]])\n----\nwhen input is [24] the target prediction is: 43\nwhen input is [24, 43] the target prediction is: 58\nwhen input is [24, 43, 58] the target prediction is: 5\nwhen input is [24, 43, 58, 5] the target prediction is: 57\nwhen input is [24, 43, 58, 5, 57] the target prediction is: 1\nwhen input is [24, 43, 58, 5, 57, 1] the target prediction is: 46\nwhen input is [24, 43, 58, 5, 57, 1, 46] the target prediction is: 43\nwhen input is [24, 43, 58, 5, 57, 1, 46, 43] the target prediction is: 39\nwhen input is [44] the target prediction is: 53\nwhen input is [44, 53] the target prediction is: 56\nwhen input is [44, 53, 56] the target prediction is: 1\nwhen input is [44, 53, 56, 1] the target prediction is: 58\nwhen input is [44, 53, 56, 1, 58] the target prediction is: 46\nwhen input is [44, 53, 56, 1, 58, 46] the target prediction is: 39\nwhen input is [44, 53, 56, 1, 58, 46, 39] the target prediction is: 58\nwhen input is [44, 53, 56, 1, 58, 46, 39, 58] the target prediction is: 1\nwhen input is [52] the target prediction is: 58\nwhen input is [52, 58] the target prediction is: 1\nwhen input is [52, 58, 1] the target prediction is: 58\nwhen input is [52, 58, 1, 58] the target prediction is: 46\nwhen input is [52, 58, 1, 58, 46] the target prediction is: 39\nwhen input is [52, 58, 1, 58, 46, 39] the target prediction is: 58\nwhen input is [52, 58, 1, 58, 46, 39, 58] the target prediction is: 1\nwhen input is [52, 58, 1, 58, 46, 39, 58, 1] the target prediction is: 46\nwhen input is [25] the target prediction is: 17\nwhen input is [25, 17] the target prediction is: 27\nwhen input is [25, 17, 27] the target prediction is: 10\nwhen input is [25, 17, 27, 10] the target prediction is: 0\nwhen input is [25, 17, 27, 10, 0] the target prediction is: 21\nwhen input is [25, 17, 27, 10, 0, 21] the target prediction is: 1\nwhen input is [25, 17, 27, 10, 0, 21, 1] the target prediction is: 54\nwhen input is [25, 17, 27, 10, 0, 21, 1, 54] the target prediction is: 39\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"## Attension","metadata":{"id":"SL0fk6ygaPRS"}},{"cell_type":"markdown","source":"- **B:** Batch Size\n- **T:** Times Dimemsion/Sequence size/block size\n- **C:**: Channel/feature size/Embedding Size (aka d(modle))","metadata":{"id":"xyLspqjpaS-v"}},{"cell_type":"markdown","source":"### Encode, Positional Encoding and then Stack","metadata":{"id":"ID2bKj6FlelS"}},{"cell_type":"code","source":"import torch.nn as nn\ntorch.manual_seed(1337)\n\n\nB,T,C = 4,8,32\n\nembeddings_table = nn.Embedding(vocab_size, C)\n\n\nembed = embeddings_table(x_batch)\n","metadata":{"id":"cV6laY9FDCCw","trusted":true,"execution":{"iopub.status.busy":"2025-07-16T09:07:06.156331Z","iopub.execute_input":"2025-07-16T09:07:06.157197Z","iopub.status.idle":"2025-07-16T09:07:06.170682Z","shell.execute_reply.started":"2025-07-16T09:07:06.157172Z","shell.execute_reply":"2025-07-16T09:07:06.169931Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"embed","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x5juXfMDlXKv","outputId":"564cf4dd-e006-4c8b-cf70-0820c2733253","trusted":true,"execution":{"iopub.status.busy":"2025-07-16T09:07:09.379462Z","iopub.execute_input":"2025-07-16T09:07:09.380242Z","iopub.status.idle":"2025-07-16T09:07:09.416840Z","shell.execute_reply.started":"2025-07-16T09:07:09.380205Z","shell.execute_reply":"2025-07-16T09:07:09.416278Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"tensor([[[ 0.3330,  1.0995,  0.4034,  ...,  1.6634, -0.4718,  0.5857],\n         [-2.2617, -0.7029, -0.1605,  ...,  0.0302,  1.6893,  1.5554],\n         [-1.7969, -0.5603,  0.9169,  ..., -0.7248, -1.0435,  1.9399],\n         ...,\n         [-0.6631, -0.2513,  1.0101,  ...,  1.5333,  1.6097, -0.4032],\n         [ 1.0729, -1.1304, -1.2815,  ..., -1.1337, -1.2783, -0.4389],\n         [-2.2617, -0.7029, -0.1605,  ...,  0.0302,  1.6893,  1.5554]],\n\n        [[-1.2172,  1.2531,  1.7768,  ...,  0.2914,  0.3634, -1.0550],\n         [-0.0201, -0.2614,  0.9901,  ...,  0.3953,  1.8599,  1.2386],\n         [-0.3012, -0.4901, -1.3679,  ...,  2.2434,  1.6029, -0.7250],\n         ...,\n         [ 1.0729, -1.1304, -1.2815,  ..., -1.1337, -1.2783, -0.4389],\n         [-0.2780, -0.4107, -1.2744,  ..., -0.2797,  0.0691,  0.2965],\n         [-1.7969, -0.5603,  0.9169,  ..., -0.7248, -1.0435,  1.9399]],\n\n        [[-0.6663,  1.2759,  0.3141,  ..., -0.7202,  0.8811, -0.2593],\n         [-1.7969, -0.5603,  0.9169,  ..., -0.7248, -1.0435,  1.9399],\n         [-0.6631, -0.2513,  1.0101,  ...,  1.5333,  1.6097, -0.4032],\n         ...,\n         [-0.2780, -0.4107, -1.2744,  ..., -0.2797,  0.0691,  0.2965],\n         [-1.7969, -0.5603,  0.9169,  ..., -0.7248, -1.0435,  1.9399],\n         [-0.6631, -0.2513,  1.0101,  ...,  1.5333,  1.6097, -0.4032]],\n\n        [[-0.9579,  0.9435, -2.1992,  ..., -0.7296,  0.1653, -0.3390],\n         [-0.2352, -0.2586,  0.0131,  ...,  0.6690,  0.7535, -0.5359],\n         [ 0.4368,  0.4685,  0.4910,  ..., -2.7447,  1.0874, -0.9978],\n         ...,\n         [-0.6896, -0.7080, -0.3152,  ..., -2.0662, -1.1418, -0.1391],\n         [-0.6631, -0.2513,  1.0101,  ...,  1.5333,  1.6097, -0.4032],\n         [-0.5447, -0.0817, -0.3788,  ..., -0.7047, -0.1852, -0.6243]]],\n       grad_fn=<EmbeddingBackward0>)"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"embed.shape # B, T, C","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z6aktj2mDCGJ","outputId":"8af9f9d3-a966-4d57-a01f-f2a27cbacba0","trusted":true,"execution":{"iopub.status.busy":"2025-07-16T09:04:09.497389Z","iopub.execute_input":"2025-07-16T09:04:09.497740Z","iopub.status.idle":"2025-07-16T09:04:09.503724Z","shell.execute_reply.started":"2025-07-16T09:04:09.497712Z","shell.execute_reply":"2025-07-16T09:04:09.502867Z"}},"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"torch.Size([4, 8, 32])"},"metadata":{}}],"execution_count":32},{"cell_type":"markdown","source":"### Find Q, K and V","metadata":{"id":"lZZaquHPllh_"}},{"cell_type":"code","source":"head_size = 16\n\nquery = nn.Linear(C, head_size, bias=False)\nkey = nn.Linear(C, head_size, bias=False)\nvalue = nn.Linear(C, head_size, bias=False)\nQ, K, V = query(embed), key(embed), value(embed)\n\nprint(Q.shape, K.shape, V.shape)\nprint(Q[0])","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RKUhCRLYkHL3","outputId":"41b5bcfe-eab8-48c0-8d40-4683394dca94","trusted":true,"execution":{"iopub.status.busy":"2025-07-16T09:07:14.485428Z","iopub.execute_input":"2025-07-16T09:07:14.485970Z","iopub.status.idle":"2025-07-16T09:07:14.512445Z","shell.execute_reply.started":"2025-07-16T09:07:14.485946Z","shell.execute_reply":"2025-07-16T09:07:14.511904Z"}},"outputs":[{"name":"stdout","text":"torch.Size([4, 8, 16]) torch.Size([4, 8, 16]) torch.Size([4, 8, 16])\ntensor([[ 0.0573, -0.1047, -0.0467, -0.1401, -0.8413, -0.1362, -0.6747, -0.2154,\n          1.0993,  0.2343,  0.0326, -0.1852,  0.1478, -0.6104,  1.5391, -0.5112],\n        [ 0.5974,  0.1158, -0.9907, -0.0327,  0.1948, -0.1020,  0.9918,  0.2401,\n          0.4138,  0.3732, -0.5734,  0.5885, -0.2606,  0.3946,  0.4797,  0.6414],\n        [ 0.3275, -0.2036,  0.1445,  0.4158,  0.8496,  0.2603, -0.1707,  0.4784,\n          0.2083, -0.1388,  0.2136,  0.1499,  0.4609,  0.3252, -0.1523,  1.5304],\n        [ 0.0578, -0.3350,  0.8477,  0.3876,  0.1664, -0.4587, -0.5974,  0.4961,\n          0.6548,  0.0548,  0.9468,  0.4511,  0.1200,  1.0573, -0.2257,  1.6714],\n        [ 0.0116, -0.5562, -0.0083,  0.2499,  0.6354,  0.8909,  0.0536, -0.4233,\n          0.1603, -0.3239,  0.3254, -0.1206,  0.0912,  0.3033, -0.2112, -0.4756],\n        [-0.8144, -0.3242,  0.5191, -0.1252, -0.4898, -0.5287, -0.0314,  0.1072,\n          0.8269,  0.8132, -0.0271,  0.4775,  0.4980, -0.1377,  1.4025, -0.8963],\n        [ 0.5261, -0.4169,  0.0085, -0.5644,  0.1517,  0.0825,  0.1202,  0.3681,\n         -0.2985, -0.4407, -0.3468,  0.2409,  0.0896,  0.1603,  0.0620,  0.1688],\n        [ 0.5974,  0.1158, -0.9907, -0.0327,  0.1948, -0.1020,  0.9918,  0.2401,\n          0.4138,  0.3732, -0.5734,  0.5885, -0.2606,  0.3946,  0.4797,  0.6414]],\n       grad_fn=<SelectBackward0>)\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"### Attension Score\n\n$score = Q⋅K^{T}$","metadata":{"id":"EP-xYJVj17US"}},{"cell_type":"code","source":"score = Q @ K.transpose(-2,-1)","metadata":{"id":"gIj7xl4EkHIX","trusted":true,"execution":{"iopub.status.busy":"2025-07-16T09:07:17.251100Z","iopub.execute_input":"2025-07-16T09:07:17.251841Z","iopub.status.idle":"2025-07-16T09:07:17.259086Z","shell.execute_reply.started":"2025-07-16T09:07:17.251815Z","shell.execute_reply":"2025-07-16T09:07:17.258431Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"### Scaling","metadata":{"id":"UHqN6guU2N_0"}},{"cell_type":"code","source":"# Scaling\nk = head_size\nscaled_score = score / (k**0.5)\nscaled_score[1]","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oK5kR5zvkHFj","outputId":"1a0570b0-3ec7-45eb-9f71-927b2608ef22","trusted":true,"execution":{"iopub.status.busy":"2025-07-16T09:07:19.209938Z","iopub.execute_input":"2025-07-16T09:07:19.210627Z","iopub.status.idle":"2025-07-16T09:07:19.216524Z","shell.execute_reply.started":"2025-07-16T09:07:19.210605Z","shell.execute_reply":"2025-07-16T09:07:19.216008Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"tensor([[ 7.2954e-02,  3.9296e-01,  7.5671e-02, -9.2254e-02, -2.2411e-01,\n         -6.7716e-01,  5.1685e-01, -2.2411e-01],\n        [ 3.4705e-01,  6.2494e-02, -5.0811e-01, -1.9011e-02,  3.6598e-01,\n          2.7219e-01, -6.2802e-01,  3.6598e-01],\n        [-2.1810e-01, -2.7876e-01,  3.2430e-01, -3.5384e-01,  1.3367e-01,\n         -1.9723e-01,  5.2815e-01,  1.3367e-01],\n        [-3.0444e-01, -9.0832e-02, -3.3691e-01, -2.6346e-01, -4.3460e-01,\n          3.1682e-01, -1.4789e-01, -4.3460e-01],\n        [ 1.1965e-01,  3.3655e-01, -1.9899e-01,  1.8603e-01,  1.0388e-01,\n         -1.2744e-02, -3.5347e-01,  1.0388e-01],\n        [-2.9451e-02, -2.3620e-01,  3.2150e-04, -6.0645e-02,  5.2209e-02,\n         -7.4563e-02, -4.4320e-02,  5.2209e-02],\n        [ 3.4812e-01,  1.3688e-01, -1.4691e-01,  3.1640e-01, -4.3598e-02,\n         -1.1152e-02, -7.5673e-01, -4.3598e-02],\n        [ 1.1965e-01,  3.3655e-01, -1.9899e-01,  1.8603e-01,  1.0388e-01,\n         -1.2744e-02, -3.5347e-01,  1.0388e-01]], grad_fn=<SelectBackward0>)"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"scaled_score.shape","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Vpr4RpXU5_7W","outputId":"ce7ebb6f-61d1-47f2-b6d3-0bb87356d223","trusted":true,"execution":{"iopub.status.busy":"2025-07-16T09:07:21.756560Z","iopub.execute_input":"2025-07-16T09:07:21.757200Z","iopub.status.idle":"2025-07-16T09:07:21.762216Z","shell.execute_reply.started":"2025-07-16T09:07:21.757174Z","shell.execute_reply":"2025-07-16T09:07:21.761482Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"torch.Size([4, 8, 8])"},"metadata":{}}],"execution_count":16},{"cell_type":"markdown","source":"### Masking","metadata":{"id":"fI7nadIT3Sec"}},{"cell_type":"code","source":"# Tril Mask\ntril = torch.tril(torch.ones(T, T))\ntril","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a9r7ua2i3UI5","outputId":"33d8e0c2-6174-4e21-c8b0-b60a574a6cf5","trusted":true,"execution":{"iopub.status.busy":"2025-07-16T09:07:24.614104Z","iopub.execute_input":"2025-07-16T09:07:24.614375Z","iopub.status.idle":"2025-07-16T09:07:24.622684Z","shell.execute_reply.started":"2025-07-16T09:07:24.614356Z","shell.execute_reply":"2025-07-16T09:07:24.621929Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n        [1., 1., 0., 0., 0., 0., 0., 0.],\n        [1., 1., 1., 0., 0., 0., 0., 0.],\n        [1., 1., 1., 1., 0., 0., 0., 0.],\n        [1., 1., 1., 1., 1., 0., 0., 0.],\n        [1., 1., 1., 1., 1., 1., 0., 0.],\n        [1., 1., 1., 1., 1., 1., 1., 0.],\n        [1., 1., 1., 1., 1., 1., 1., 1.]])"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"masked_scaled_score = scaled_score.masked_fill(tril == 0, float('-inf'))\nmasked_scaled_score[1]","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qbX03k7D5DX6","outputId":"f2667f17-8df1-4c0b-be11-7d81d4854abe","trusted":true,"execution":{"iopub.status.busy":"2025-07-16T09:07:27.008082Z","iopub.execute_input":"2025-07-16T09:07:27.008351Z","iopub.status.idle":"2025-07-16T09:07:27.019171Z","shell.execute_reply.started":"2025-07-16T09:07:27.008331Z","shell.execute_reply":"2025-07-16T09:07:27.018409Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"tensor([[ 7.2954e-02,        -inf,        -inf,        -inf,        -inf,\n                -inf,        -inf,        -inf],\n        [ 3.4705e-01,  6.2494e-02,        -inf,        -inf,        -inf,\n                -inf,        -inf,        -inf],\n        [-2.1810e-01, -2.7876e-01,  3.2430e-01,        -inf,        -inf,\n                -inf,        -inf,        -inf],\n        [-3.0444e-01, -9.0832e-02, -3.3691e-01, -2.6346e-01,        -inf,\n                -inf,        -inf,        -inf],\n        [ 1.1965e-01,  3.3655e-01, -1.9899e-01,  1.8603e-01,  1.0388e-01,\n                -inf,        -inf,        -inf],\n        [-2.9451e-02, -2.3620e-01,  3.2150e-04, -6.0645e-02,  5.2209e-02,\n         -7.4563e-02,        -inf,        -inf],\n        [ 3.4812e-01,  1.3688e-01, -1.4691e-01,  3.1640e-01, -4.3598e-02,\n         -1.1152e-02, -7.5673e-01,        -inf],\n        [ 1.1965e-01,  3.3655e-01, -1.9899e-01,  1.8603e-01,  1.0388e-01,\n         -1.2744e-02, -3.5347e-01,  1.0388e-01]], grad_fn=<SelectBackward0>)"},"metadata":{}}],"execution_count":18},{"cell_type":"markdown","source":"### Apply Softmax","metadata":{"id":"F-k3Jukd3G20"}},{"cell_type":"code","source":"# softmax\nsoftmax = nn.Softmax(dim=-1)\nattention = softmax(masked_scaled_score)","metadata":{"id":"N07FeaHzkHC3","trusted":true,"execution":{"iopub.status.busy":"2025-07-16T09:07:29.553843Z","iopub.execute_input":"2025-07-16T09:07:29.554125Z","iopub.status.idle":"2025-07-16T09:07:29.559338Z","shell.execute_reply.started":"2025-07-16T09:07:29.554107Z","shell.execute_reply":"2025-07-16T09:07:29.558692Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# Heatmap\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(8,8))\nplt.imshow(attention[1].detach().numpy(), cmap='Blues')","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":693},"id":"g9QovR4J6KTx","outputId":"15bc17a6-8025-468d-b89f-ec4f68f41e7e","trusted":true,"execution":{"iopub.status.busy":"2025-07-16T09:07:31.666012Z","iopub.execute_input":"2025-07-16T09:07:31.666240Z","iopub.status.idle":"2025-07-16T09:07:31.938292Z","shell.execute_reply.started":"2025-07-16T09:07:31.666224Z","shell.execute_reply":"2025-07-16T09:07:31.937526Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"<matplotlib.image.AxesImage at 0x7b3cdbc79650>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 800x800 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":20},{"cell_type":"markdown","source":"### Compute Attension\n","metadata":{"id":"xuWlQAAG6yC2"}},{"cell_type":"code","source":"# Attension\nout = attention @ V # dot product\nout.shape","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"erx0F1lLkHAO","outputId":"82e07d49-e129-4801-c5be-948b3fd1aa5b","trusted":true,"execution":{"iopub.status.busy":"2025-07-16T09:07:35.073340Z","iopub.execute_input":"2025-07-16T09:07:35.074094Z","iopub.status.idle":"2025-07-16T09:07:35.081092Z","shell.execute_reply.started":"2025-07-16T09:07:35.074063Z","shell.execute_reply":"2025-07-16T09:07:35.080275Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"torch.Size([4, 8, 16])"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"out[1]","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wdNkZVQekG9Z","outputId":"2cdabbfd-4340-4cae-f089-d5c0def96190","trusted":true,"execution":{"iopub.status.busy":"2025-07-16T09:07:37.362959Z","iopub.execute_input":"2025-07-16T09:07:37.363428Z","iopub.status.idle":"2025-07-16T09:07:37.369453Z","shell.execute_reply.started":"2025-07-16T09:07:37.363408Z","shell.execute_reply":"2025-07-16T09:07:37.368847Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"tensor([[ 0.7038, -0.2393,  0.1082,  0.0802,  0.1758, -0.3031, -0.2811, -0.0662,\n         -0.0246, -1.1509, -0.3643,  0.8445,  0.1204, -0.0039, -0.5436,  0.0952],\n        [ 0.6041,  0.0715,  0.0637,  0.0128,  0.1081, -0.4676, -0.3439, -0.1857,\n          0.1801, -0.8234, -0.3639,  0.6568, -0.2618, -0.1186, -0.3658,  0.1868],\n        [ 0.5963, -0.0171,  0.4070, -0.0533, -0.0668, -0.1588,  0.0097,  0.4589,\n          0.2787,  0.4873, -0.3234,  0.4313,  0.0235, -0.1688, -0.0862, -0.0367],\n        [ 0.6535,  0.0307,  0.3434,  0.2235, -0.3117, -0.2314, -0.1810,  0.0784,\n          0.4105,  0.2049,  0.0404,  0.5428, -0.2559, -0.1660, -0.1689, -0.0897],\n        [ 0.4461,  0.0357,  0.1528,  0.2125, -0.2955, -0.3859, -0.1908, -0.1349,\n          0.2102, -0.0318,  0.0901,  0.5216, -0.3435, -0.1452, -0.2329, -0.1251],\n        [ 0.3609, -0.1023,  0.1663,  0.2015, -0.2236, -0.1596,  0.0185, -0.0812,\n          0.0763,  0.1161,  0.0850,  0.5569, -0.0988,  0.0185, -0.2997,  0.0143],\n        [ 0.3860, -0.0906,  0.1323,  0.2604, -0.2105, -0.1629,  0.0341, -0.1152,\n          0.0837, -0.0186,  0.1524,  0.4736, -0.1727,  0.0053, -0.3588,  0.0809],\n        [ 0.2310, -0.0563,  0.0034,  0.2213, -0.1803, -0.2846,  0.0524, -0.2115,\n         -0.0462, -0.1114,  0.1583,  0.3896, -0.2573, -0.0091, -0.3763,  0.0673]],\n       grad_fn=<SelectBackward0>)"},"metadata":{}}],"execution_count":22},{"cell_type":"markdown","source":"### Putting atttension all together","metadata":{"id":"gSJiToMH905k"}},{"cell_type":"code","source":"class SelfAttension(nn.Module):\n  def __init__(self, head_size, vocab_size):\n    super().__init__()\n\n    self.head_size = head_size\n    self.vocab_size = vocab_size\n    self.C = 32\n\n    # set seed\n    torch.manual_seed(1337)\n\n\n    self.embeddings_table = nn.Embedding(self.vocab_size, self.C)\n\n    self.query = nn.Linear(C, self.head_size, bias=False)\n    self.key = nn.Linear(C, self.head_size, bias=False)\n    self.value = nn.Linear(C, self.head_size, bias=False)\n\n  def forward(self, x):\n    embed = self.embeddings_table(x)\n\n    B,T,C = embed.shape\n\n    # Find Q, K and V\n    Q, K, V = self.query(embed), self.key(embed), self.value(embed)\n\n    # Find score\n    score = Q @ K.transpose(-2,-1)\n\n    # Scale it\n    k = self.head_size\n    scaled_score = score / (k**0.5)\n\n    # Masking\n    tril = torch.tril(torch.ones(T, T))\n    masked_scaled_score = scaled_score.masked_fill(tril == 0, float('-inf'))\n\n    # Apply softmax\n    softmax = nn.Softmax(dim=-1)\n    attention = softmax(masked_scaled_score)\n\n    # Attension\n    out = attention @ V\n\n    return out\n\n","metadata":{"id":"KiAL4FSYkG7T","trusted":true,"execution":{"iopub.status.busy":"2025-07-16T09:07:39.635611Z","iopub.execute_input":"2025-07-16T09:07:39.636279Z","iopub.status.idle":"2025-07-16T09:07:39.642541Z","shell.execute_reply.started":"2025-07-16T09:07:39.636255Z","shell.execute_reply":"2025-07-16T09:07:39.641904Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"# Example usage self attention\nself_attention = SelfAttension(head_size=16, vocab_size=65)\nout = self_attention.forward(x_batch)\n\nprint(\"Shape: \", out.shape)\nprint(\"Attesntion:\\n\", out[1])","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9Tt-RTfVkG4k","outputId":"bd38e845-1caa-4678-982c-77ea46a441be","trusted":true,"execution":{"iopub.status.busy":"2025-07-16T09:07:43.936797Z","iopub.execute_input":"2025-07-16T09:07:43.937086Z","iopub.status.idle":"2025-07-16T09:07:43.946192Z","shell.execute_reply.started":"2025-07-16T09:07:43.937068Z","shell.execute_reply":"2025-07-16T09:07:43.944904Z"}},"outputs":[{"name":"stdout","text":"Shape:  torch.Size([4, 8, 16])\nAttesntion:\n tensor([[ 0.7038, -0.2393,  0.1082,  0.0802,  0.1758, -0.3031, -0.2811, -0.0662,\n         -0.0246, -1.1509, -0.3643,  0.8445,  0.1204, -0.0039, -0.5436,  0.0952],\n        [ 0.6041,  0.0715,  0.0637,  0.0128,  0.1081, -0.4676, -0.3439, -0.1857,\n          0.1801, -0.8234, -0.3639,  0.6568, -0.2618, -0.1186, -0.3658,  0.1868],\n        [ 0.5963, -0.0171,  0.4070, -0.0533, -0.0668, -0.1588,  0.0097,  0.4589,\n          0.2787,  0.4873, -0.3234,  0.4313,  0.0235, -0.1688, -0.0862, -0.0367],\n        [ 0.6535,  0.0307,  0.3434,  0.2235, -0.3117, -0.2314, -0.1810,  0.0784,\n          0.4105,  0.2049,  0.0404,  0.5428, -0.2559, -0.1660, -0.1689, -0.0897],\n        [ 0.4461,  0.0357,  0.1528,  0.2125, -0.2955, -0.3859, -0.1908, -0.1349,\n          0.2102, -0.0318,  0.0901,  0.5216, -0.3435, -0.1452, -0.2329, -0.1251],\n        [ 0.3609, -0.1023,  0.1663,  0.2015, -0.2236, -0.1596,  0.0185, -0.0812,\n          0.0763,  0.1161,  0.0850,  0.5569, -0.0988,  0.0185, -0.2997,  0.0143],\n        [ 0.3860, -0.0906,  0.1323,  0.2604, -0.2105, -0.1629,  0.0341, -0.1152,\n          0.0837, -0.0186,  0.1524,  0.4736, -0.1727,  0.0053, -0.3588,  0.0809],\n        [ 0.2310, -0.0563,  0.0034,  0.2213, -0.1803, -0.2846,  0.0524, -0.2115,\n         -0.0462, -0.1114,  0.1583,  0.3896, -0.2573, -0.0091, -0.3763,  0.0673]],\n       grad_fn=<SelectBackward0>)\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"# Multihead attention\nclass MultiHeadAttention(nn.Module):\n  def __init__(self, num_heads, head_size, vocab_size):\n    super().__init__()\n    self.heads = nn.ModuleList([SelfAttension(head_size, vocab_size) for _ in range(num_heads)])\n    self.proj = nn.Linear(head_size * num_heads, vocab_size)\n\n  def forward(self, x):\n    out = torch.cat([h(x) for h in self.heads], dim=-1)\n    out = self.proj(out)\n    return out\n\n","metadata":{"id":"9WlG6m-9Geg9","trusted":true,"execution":{"iopub.status.busy":"2025-07-16T09:07:46.523342Z","iopub.execute_input":"2025-07-16T09:07:46.524008Z","iopub.status.idle":"2025-07-16T09:07:46.528280Z","shell.execute_reply.started":"2025-07-16T09:07:46.523984Z","shell.execute_reply":"2025-07-16T09:07:46.527680Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# USe multi head attention\nmulti_head_attention = MultiHeadAttention(num_heads=4, head_size=16, vocab_size=65)\nout = multi_head_attention.forward(x_batch)\n\nprint(\"Shape: \", out.shape)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mZV51Yq3Gnen","outputId":"35de8cab-c809-4d4c-ede4-a6ff2fa27a8b","trusted":true,"execution":{"iopub.status.busy":"2025-07-16T09:07:48.626976Z","iopub.execute_input":"2025-07-16T09:07:48.627212Z","iopub.status.idle":"2025-07-16T09:07:48.642231Z","shell.execute_reply.started":"2025-07-16T09:07:48.627195Z","shell.execute_reply":"2025-07-16T09:07:48.641570Z"}},"outputs":[{"name":"stdout","text":"Shape:  torch.Size([4, 8, 65])\n","output_type":"stream"}],"execution_count":26},{"cell_type":"markdown","source":"# ----------------- Model ------------------","metadata":{"id":"vu7q90qtowfQ"}},{"cell_type":"markdown","source":"## Simple Bigram Model","metadata":{"id":"ix-Ksc0L5Qvl"}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\ntorch.manual_seed(1337)\n\n\nclass BigramLanguageModel(nn.Module):\n  def __init__(self, vocab_size):\n    super().__init__()\n\n    # Let say we have tokens and embed them\n    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n  def forward(self, idx, targets=None):\n    \"\"\"Perform processing here.\n    idx:- (B,T) tensor of integer.\n    targets:- (B,T) tensor of integer.\n    \"\"\"\n\n\n    logits = self.token_embedding_table(idx) # (B,T,C)\n\n    return logits\n\n\nm = BigramLanguageModel(vocab_size)\nout = m(x_batch, y_batch)\nprint(out.shape)","metadata":{"id":"MDygCkDWkG1Q","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7f8f311c-79fa-4900-e42f-82bb8da17e46","trusted":true,"execution":{"iopub.status.busy":"2025-07-16T09:07:50.373281Z","iopub.execute_input":"2025-07-16T09:07:50.373810Z","iopub.status.idle":"2025-07-16T09:07:50.381514Z","shell.execute_reply.started":"2025-07-16T09:07:50.373790Z","shell.execute_reply":"2025-07-16T09:07:50.380819Z"}},"outputs":[{"name":"stdout","text":"torch.Size([4, 8, 65])\n","output_type":"stream"}],"execution_count":27},{"cell_type":"markdown","source":"This is very simple model and we can easily find next charecter prediction as did in LexiGen in my previous project.\nNow we need to define loss to check the quality.","metadata":{"id":"63eFIbCbqOLn"}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\ntorch.manual_seed(1337)\n\n\nclass BigramLanguageModel(nn.Module):\n  def __init__(self, vocab_size):\n    super().__init__()\n\n    # Let say we have tokens and embed them\n    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n  def forward(self, idx, targets=0):\n    \"\"\"Perform processing here.\n    idx:- (B,T) tensor of integer.\n    targets:- (B,T) tensor of integer.\n    \"\"\"\n    # Logits\n    logits = self.token_embedding_table(idx) # (B,T,C)\n\n    # Loss function\n    # We using cross entropy loss and it accepts data in B,C,T shape thats why we need some minor changings.\n    if targets is None:\n      loss = None\n    else:\n      B, T, C = logits.shape\n      logits = logits.view(B*T, C)\n      targets = targets.view(B*T)\n      loss = F.cross_entropy(logits, targets)\n\n    return {\n        'logits': logits.shape,\n        'loss': loss\n    }\n\n\nm = BigramLanguageModel(vocab_size)\nout = m(x_batch, y_batch)\nprint(out)","metadata":{"id":"6FbTJ8YgkGyg","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0b97ce73-519c-43aa-f118-844409dbb404","trusted":true,"execution":{"iopub.status.busy":"2025-07-16T09:07:53.311393Z","iopub.execute_input":"2025-07-16T09:07:53.311911Z","iopub.status.idle":"2025-07-16T09:07:53.324498Z","shell.execute_reply.started":"2025-07-16T09:07:53.311879Z","shell.execute_reply":"2025-07-16T09:07:53.323821Z"}},"outputs":[{"name":"stdout","text":"{'logits': torch.Size([32, 65]), 'loss': tensor(4.8786, grad_fn=<NllLossBackward0>)}\n","output_type":"stream"}],"execution_count":28},{"cell_type":"markdown","source":"We have 65 possible vocab elements so:\n\n-ln(1/65) = 4.1743\n\nThis is the loss we are axpecting, it means our models is guessing wrong.","metadata":{"id":"gDmPXwhLy087"}},{"cell_type":"markdown","source":"Now, we are going to get some output from our mdoel to check how well it is prediction.","metadata":{"id":"Wu8gDh2zzgey"}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\ntorch.manual_seed(1337)\n\n\nclass BigramLanguageModel(nn.Module):\n  def __init__(self, vocab_size):\n    super().__init__()\n\n    # Let say we have tokens and embed them\n    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n  def forward(self, idx, targets=None):\n    \"\"\"Perform processing here.\n    idx:- (B,T) tensor of integer.\n    targets:- (B,T) tensor of integer.\n    \"\"\"\n    # Logits\n    logits = self.token_embedding_table(idx) # (B,T,C)\n\n    # Loss function\n    # We using cross entropy loss and it accepts data in B,C,T shape thats why we need some minor changings.\n    if targets is None:\n      loss = None\n    else:\n      B, T, C = logits.shape\n      logits = logits.view(B*T, C)\n      targets = targets.view(B*T)\n      loss = F.cross_entropy(logits, targets)\n\n    return logits, loss\n\n  def generate(self, idx, max_new_tokens):\n\n    for _ in range(max_new_tokens):\n      # get predictions (logits)\n      logits, loss = self(idx) # Calling forward function.\n\n      logits = logits[:, -1, :] # (B,C)\n      probs = F.softmax(logits, dim=-1) # (B,C)\n      idx_next = torch.multinomial(probs, num_samples=1) # (B,1)\n      idx = torch.cat((idx, idx_next), dim=1) # (B,T+1)\n\n    return idx\n\n\nm = BigramLanguageModel(vocab_size)\n# idx is 0  and zero equals . which is our starting point of a word.\nidx = torch.zeros((1,1), dtype=torch.long)\n# Genrate new tokens which will be encoding and we shall decode them e.g 0 --> . 1 --> a etc.\nprint(decode(m.generate(idx, max_new_tokens=100)[0].tolist()))","metadata":{"id":"3kCQF4HJkGwH","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9dc49fec-1d5f-4192-dc66-abf2a155e5c6","trusted":true,"execution":{"iopub.status.busy":"2025-07-16T09:07:56.238465Z","iopub.execute_input":"2025-07-16T09:07:56.239314Z","iopub.status.idle":"2025-07-16T09:07:56.275219Z","shell.execute_reply.started":"2025-07-16T09:07:56.239288Z","shell.execute_reply":"2025-07-16T09:07:56.274654Z"}},"outputs":[{"name":"stdout","text":"\nSr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n","output_type":"stream"}],"execution_count":29},{"cell_type":"markdown","source":"yeah! its working but...... Garbage.","metadata":{"id":"8SyK8TdC3dHy"}},{"cell_type":"markdown","source":"## Training","metadata":{"id":"4WcKESFD3_Q6"}},{"cell_type":"markdown","source":"Optimizer","metadata":{"id":"TGyvqaYh4CKk"}},{"cell_type":"code","source":"optim = torch.optim.AdamW(m.parameters(), lr=1e-3)","metadata":{"id":"rgVkJ5BC3dbV","trusted":true,"execution":{"iopub.status.busy":"2025-07-16T09:07:59.644394Z","iopub.execute_input":"2025-07-16T09:07:59.644673Z","iopub.status.idle":"2025-07-16T09:08:02.116724Z","shell.execute_reply.started":"2025-07-16T09:07:59.644652Z","shell.execute_reply":"2025-07-16T09:08:02.116188Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"# Training loop\nfor steps in range(10000):\n\n  # sample a btach\n  xb, yb = get_batch('train')\n\n  # evaluate loss\n  logits, loss = m(xb, yb) # Forward is callable.\n  optim.zero_grad(set_to_none=True)\n  loss.backward()\n  optim.step()\n\nprint(loss.item())","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f5ER8xFV3dd5","outputId":"f29f4106-561a-4c86-a8f8-7828e18e3a48","trusted":true,"execution":{"iopub.status.busy":"2025-07-16T09:08:03.525988Z","iopub.execute_input":"2025-07-16T09:08:03.526609Z","iopub.status.idle":"2025-07-16T09:08:10.900717Z","shell.execute_reply.started":"2025-07-16T09:08:03.526585Z","shell.execute_reply":"2025-07-16T09:08:10.899950Z"}},"outputs":[{"name":"stdout","text":"2.4212486743927\n","output_type":"stream"}],"execution_count":31},{"cell_type":"markdown","source":"## Prediction","metadata":{"id":"Icu5LeDv5V7Z"}},{"cell_type":"code","source":"# idx is 0  and zero equals . which is our starting point of a word.\nidx = torch.zeros((1,1), dtype=torch.long)\n# Genrate new tokens which will be encoding and we shall decode them e.g 0 --> . 1 --> a etc.\nprint(decode(m.generate(idx, max_new_tokens=1000)[0].tolist()))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o1ean72z3dgr","outputId":"ea83084e-ea24-45aa-db06-484bc2b3ed17","trusted":true,"execution":{"iopub.status.busy":"2025-07-16T09:08:13.817147Z","iopub.execute_input":"2025-07-16T09:08:13.817382Z","iopub.status.idle":"2025-07-16T09:08:13.953272Z","shell.execute_reply.started":"2025-07-16T09:08:13.817366Z","shell.execute_reply":"2025-07-16T09:08:13.952555Z"}},"outputs":[{"name":"stdout","text":"\nKIZ!\n\nwe?\nW$d he k\nMIV;\n\n\nIETyoZY st:\n\n\nGheay Yatondgrende'TEQUNathinin: ans nd s\n\nOREre.\nQhour im, sof bn I's temrpe thithattersuravWhefe h antr forgg m walos nt kit mo RLORZ'd IFknyor!\nSindis prewhous oodod.\nwouprebe bll:CUNTHUSt\n\nThleaix;\nMalesheais whe; lino.\nALI;\nCXdaimboth,\nbatly, tyous w F nig har-\nASerin livin ad:\nGRveitho wond ve ack'\n3'd gat held inghe beromUCOFFoce ?Y's.\nPat fe, healcofoan.\nFXgrnk3ffstoncetasollar dir ce s:\n\nFGS OXELEurdo the horer mf IOP,-W?ENe,\nLIDbor d,\n\nCUSY: CKr t k the, averofithanontowbr boute, antorg ithe,\nDndill d,loworathalichothazLINothipr sowserll!-bee be y VObe, st,\nU'l, ifrenit w fo-ber, OFo berad wit t w$itenepUING?cofe Win.\nCudsce kelind seail, is.\nARJathee p st; YNbs rld pr, auf tauth ny l bar:\ncee hes ndetasuse\n\nIno'&ckinthe Wre tan; anu be, mZSEYQumbu wengboitor wismabe dat\nIUN:\nAUNoveed'stathemy;D:\nwel batyamiPves;\nLIthe hat b, ooly ingr w, th ar foumyo s ere fomswouf lSJu r f II ot wen rknge oorenn m ho w Sie l, amaveve a?Bu p nyoncllolm\n","output_type":"stream"}],"execution_count":32},{"cell_type":"markdown","source":"Loss decreased thats why getting something reasonable.","metadata":{"id":"mF73kYzk45-t"}},{"cell_type":"markdown","source":"## Combining All","metadata":{"id":"zvKNU1t28aJt"}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\n# Set seed\ntorch.manual_seed(1337)\n\n# Hyperparameters\nblock_size = 32\nbatch_size = 64\nmax_iters = 50000\neval_interval = 100\nlearning_rate = 1e-3\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# Dataset\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\n#  Preprocess\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\n\n\n# Encode Decode\nstring_to_int = { ch:i for i,ch in enumerate(chars) }\nint_to_string = { i:ch for i,ch in enumerate(chars) }\n\n\n# Split data\ndata = torch.tensor(encode(text), dtype=torch.long)\nn = int(0.9*len(data))\ntrain_data = data[:n]\nval_data = data[n:]\n\n\n# Loading data\ndef get_batch(split):\n  \"\"\"Generate batch of data input x and target y.\"\"\"\n  data = train_data if split == 'train' else val_data\n  ix = torch.randint(len(data) - block_size, (batch_size,))\n  x = torch.stack([data[i:i+block_size] for i in ix])\n  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n  return x, y\n\n# Loss estimating\n@torch.no_grad()\ndef estimate_loss():\n  out = {}\n  m.eval()\n  for split in ['train', 'val']:\n    losses = torch.zeros(eval_interval)\n    for k in range(eval_interval):\n      X, Y = get_batch(split)\n      logits, loss = m(X, Y)\n      losses[k] = loss.item()\n    out[split] = losses.mean()\n  m.train()\n  return out\n\n\n# Model\nclass BigramLanguageModel(nn.Module):\n  def __init__(self, vocab_size):\n    super().__init__()\n\n    # Let say we have tokens and embed them\n    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n  def forward(self, idx, targets=None):\n    \"\"\"Perform processing here.\n    idx:- (B,T) tensor of integer.\n    targets:- (B,T) tensor of integer.\n    \"\"\"\n    # Logits\n    logits = self.token_embedding_table(idx) # (B,T,C)\n\n    # Loss function\n    # We using cross entropy loss and it accepts data in B,C,T shape thats why we need some minor changings.\n    if targets is None:\n      loss = None\n    else:\n      B, T, C = logits.shape\n      logits = logits.view(B*T, C)\n      targets = targets.view(B*T)\n      loss = F.cross_entropy(logits, targets)\n\n    return logits, loss\n\n  def generate(self, idx, max_new_tokens):\n\n    for _ in range(max_new_tokens):\n      # get predictions (logits)\n      logits, loss = self(idx) # Calling forward function.\n\n      logits = logits[:, -1, :] # (B,C)\n      probs = F.softmax(logits, dim=-1) # (B,C)\n      idx_next = torch.multinomial(probs, num_samples=1) # (B,1)\n      idx = torch.cat((idx, idx_next), dim=1) # (B,T+1)\n\n    return idx\n\n\n# Optimizer\noptim = torch.optim.AdamW(m.parameters(), lr=1e-3)\n\n# Training\nm = BigramLanguageModel(vocab_size)\nfor steps in range(max_iters):\n\n  # Estimate loss\n  if steps % 10000 == 0:\n    losses = estimate_loss()\n    print(f\"step {steps}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n\n  # sample a btach\n  xb, yb = get_batch('train')\n  logits, loss = m(xb, yb) # Forward is callable.\n  optim.zero_grad(set_to_none=True)\n  loss.backward()\n  optim.step()\n\n\n# Prediction\n# idx is 0  and zero equals . which is our starting point of a word.\nidx = torch.zeros((1,1), dtype=torch.long)\n# Genrate new tokens which will be encoding and we shall decode them e.g 0 --> . 1 --> a etc.\nprint(decode(m.generate(idx, max_new_tokens=1000)[0].tolist()))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GZx1s7xi3djH","outputId":"5d9421c2-42ee-4202-8ff3-3090748a76f9","trusted":true,"execution":{"iopub.status.busy":"2025-07-16T09:08:29.374403Z","iopub.execute_input":"2025-07-16T09:08:29.374660Z","iopub.status.idle":"2025-07-16T09:10:48.208692Z","shell.execute_reply.started":"2025-07-16T09:08:29.374641Z","shell.execute_reply":"2025-07-16T09:10:48.207939Z"}},"outputs":[{"name":"stdout","text":"step 0: train loss 4.7261, val loss 4.7230\nstep 10000: train loss 4.7266, val loss 4.7236\nstep 20000: train loss 4.7251, val loss 4.7239\nstep 30000: train loss 4.7274, val loss 4.7243\nstep 40000: train loss 4.7279, val loss 4.7260\n\np&ZAd jPylvV&bfFimy,iWZgF$-uxj,MJAU?zLjufB-tSBpG-jxhcLT?fxIb&yeathT:kpUNCJXlIoB'QCj$lSql\ny,SA'YGGXlIkpupHhE$SlzaN:CF$zRJjsTsBTdF;QCH;AHfkyXN!aADEHEJWymnOjxltRT-ttr$AcMpdlS.Sq,ivkzbD.P -bLEjZJd,Sq-hylvSdzyA:xQhKB-woi?yhsSrU,v!Jd!:Ia!UNCttaQQEH,SGFjKh fRjQK&iM,phjd; kNWwW?pJreDCjgi?,iyO'RpADLUda!mDnk:C-wKH-Qg!m,VNpXEev'ymVOy ?wh$xtDHHxlzl-QTOqf$UAcL&dmFdmvqVbOcPLgmN'lgOXfu!ChOwrVI!JMe?ObhT;:wXIJd?nhEUEtd?.ev$kc;abRjmbXEnTQL-oTV;JtR:vWNw;YYhae?a!e'a- !dddYTEHA-olxa!zPpvbZ!oboRSNC vir,&X&Bju!Bdm.IKMC a-vvmmvyNA:CjFq,ShP'UsMm$wzT CgrKVrJFVw$ \nqgLTn!U?hJ!qzLs:3Qkzd?q?smpZTaHxJWWkz;.\nX!za;voicotN bohK$!$G,OGj;ADMIHkO IB\nocLs$gCU'\n&;AU!eZVbsA!Og$mZdLfl-KpgiWnxr&Q$UsBk!USV'a!FEPgCTHV?YZa!PgIQUqo?nOXLJiP'gCVz;lZItR:BHttoTaRjQJBoCXla\nC f&ljyeArP!B3KSbfxIgZRvijmb\n$-UzeGSq$VvSxZA$ukoCOWgi?XkOq-ms$FRwB'UzB-a-qKHWghRC ,j;ARibOlviq,;h?3d&fFl?ogWIczbRJUApnkq-IdEwXAPy,;zeZnYCD.Rj\nSYIBhi$-aQ?nE$GCDh?qdaepHotUErLviRkhC&vcL-EvvW,S.EXi\n!sriMmGRLVxl,l;Q&yhaZKRTbePdvUNXyf?wx\nvWpJdmRSPwWxhr:Cdmc'iT$T'j,jlyZhre\n","output_type":"stream"}],"execution_count":33},{"cell_type":"markdown","source":"## Adding previosuly created attesnion","metadata":{"id":"iVJjLzKbLnil"}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\n# Set seed\ntorch.manual_seed(1337)\n\n# Hyperparameters\nblock_size = 32\nbatch_size = 64\nmax_iters = 100_000\neval_interval = 1000\nlearning_rate = 1e-3\nn_embed = 32\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"Using device: {device}\")\n\n# Dataset\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\n# Preprocess\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\n\n# Encode Decode\nstring_to_int = { ch:i for i,ch in enumerate(chars) }\nint_to_string = { i:ch for i,ch in enumerate(chars) }\nencode = lambda s: [string_to_int[c] for c in s]\ndecode = lambda l: ''.join([int_to_string[i] for i in l])\n\n# Split data\ndata = torch.tensor(encode(text), dtype=torch.long)\nn = int(0.9*len(data))\ntrain_data = data[:n]\nval_data = data[n:]\n\n# Loading data\ndef get_batch(split):\n    \"\"\"Generate batch of data input x and target y.\"\"\"\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    return x.to(device), y.to(device)  # Moved to device\n\n# Loss estimating\n@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    m.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_interval)\n        for k in range(eval_interval):\n            X, Y = get_batch(split)  # Already on device\n            logits, loss = m(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    m.train()\n    return out\n\n# Attention\nclass Head(nn.Module):\n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(n_embed, head_size, bias=False)\n        self.query = nn.Linear(n_embed, head_size, bias=False)\n        self.value = nn.Linear(n_embed, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n\n    def forward(self, x):\n        B, T, C = x.shape\n        Q, K, V = self.query(x), self.key(x), self.value(x)\n        scaled_score = Q @ K.transpose(-2,-1) * (C**-0.5)\n        masked_scaled_score = scaled_score.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n        attention = F.softmax(masked_scaled_score, dim=-1)\n        out = attention @ V\n        return out\n\n# Model\nclass BigramLanguageModel(nn.Module):\n    def __init__(self, vocab_size):\n        super().__init__()\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n        self.self_att_head = Head(n_embed)\n        self.lm_head = nn.Linear(n_embed, vocab_size)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n        token_embed = self.token_embedding_table(idx)\n        positions = torch.arange(T, device=device)  # Created on device\n        position_embed = self.position_embedding_table(positions)\n        x = token_embed + position_embed\n        attn_out = self.self_att_head(x)\n        logits = self.lm_head(attn_out)\n        \n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        for _ in range(max_new_tokens):\n            idx_cond = idx[:, -block_size:]\n            logits, loss = self(idx_cond)\n            logits = logits[:, -1, :]\n            probs = F.softmax(logits, dim=-1)\n            idx_next = torch.multinomial(probs, num_samples=1)\n            idx = torch.cat((idx, idx_next), dim=1)\n        return idx\n\n# Initialize model and move to device\nm = BigramLanguageModel(vocab_size).to(device)  # Model moved to device\noptim = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n\n# Training loop\nfor steps in range(max_iters):\n    if steps % 10000 == 0:\n        losses = estimate_loss()\n        print(f\"step {steps}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n    \n    xb, yb = get_batch('train')  # Already on device\n    logits, loss = m(xb, yb)\n    optim.zero_grad(set_to_none=True)\n    loss.backward()\n    optim.step()\n\n# Final loss\nlosses = estimate_loss()\nprint(f\"Final: step {steps}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n\n# Generation\nidx = torch.zeros((1, 1), dtype=torch.long, device=device)  # Created on device\nprint(decode(m.generate(idx, max_new_tokens=1000)[0].tolist()))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AlMwjc3i3dl5","outputId":"4180a1f3-50ba-4337-ccbd-164535c9ee49","trusted":true,"execution":{"iopub.status.busy":"2025-07-16T09:36:28.351244Z","iopub.execute_input":"2025-07-16T09:36:28.351813Z","iopub.status.idle":"2025-07-16T09:43:30.062365Z","shell.execute_reply.started":"2025-07-16T09:36:28.351792Z","shell.execute_reply":"2025-07-16T09:43:30.061523Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nstep 0: train loss 4.1756, val loss 4.1771\nstep 10000: train loss 2.3003, val loss 2.3331\nstep 20000: train loss 2.2829, val loss 2.3240\nstep 30000: train loss 2.2726, val loss 2.3242\nstep 40000: train loss 2.2681, val loss 2.3201\nstep 50000: train loss 2.2649, val loss 2.3166\nstep 60000: train loss 2.2591, val loss 2.3147\nstep 70000: train loss 2.2591, val loss 2.3168\nstep 80000: train loss 2.2553, val loss 2.3166\nstep 90000: train loss 2.2547, val loss 2.3140\nFinal: step 99999: train loss 2.2529, val loss 2.3096\n\nANUnt iknod\nOlowr,\nThis by bth\nYad the obe toe.\nSthand my dalilans bar bthar us hathe. Par dilas ate awice my.\n\nHDY:\nYo Grem he owno, tof it hy me mil ndill, aes iree sen cie lat Het drovets, and Wil ngan ilerans!\nelplind meal.\nHAMET:\nSchiry: tupr aiss hawty. Junto.\nIn Boupeen aves\nMomy.\nWhod mothake ont---ond do eiizes the my,\nCHil The higend thoous\nAl-\nThe\nTo kind thref If son; igr the:\nEr Cip ale ont ffaf Pre?\n\nWARKINUSHTING a!\n\nWARDUKK:\nSadsad the Ere stohuin cour ayr tey If his chan yo voulken pand, bemary.\nYou cont way mes ha anghs toe nomepet renet.\n\nCORY ICHENREWARGCERNOMugi ome.\n\nThuce ff py tshintchi lom my.\nMomont aleingu.\n\nLAWISSTA:\nThalvet: im pas!\n\nWARICE:\nMy vit stie st ums histe fe' lates:\nWhit Clo hscesn?\n\nYORK:\nGod ton, moxme acheanthatakes agh whein\n'As: th sleve buel necod the wllo no'nd id, mors honourd?\nTI idurd porvenand, dw thieyr ivethe of tiund tho nof the sut ne apryou on whanden sitth ougl peime ollke, on sothan thean, chewrt hotive wout igt thu;\nyo knogin. \n","output_type":"stream"}],"execution_count":46},{"cell_type":"markdown","source":"## Multi Head Attension","metadata":{}},{"cell_type":"markdown","source":"Until now we were adding single head attenstion, but now we shall add multi heads of attension.","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\n# Set seed\ntorch.manual_seed(1337)\n\n# Hyperparameters\nblock_size = 32\nbatch_size = 64\nmax_iters = 100_000\neval_interval = 1000\nlearning_rate = 1e-3\nn_embed = 32\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"Using device: {device}\")\n\n# Dataset\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\n# Preprocess\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\n\n# Encode Decode\nstring_to_int = { ch:i for i,ch in enumerate(chars) }\nint_to_string = { i:ch for i,ch in enumerate(chars) }\nencode = lambda s: [string_to_int[c] for c in s]\ndecode = lambda l: ''.join([int_to_string[i] for i in l])\n\n# Split data\ndata = torch.tensor(encode(text), dtype=torch.long)\nn = int(0.9*len(data))\ntrain_data = data[:n]\nval_data = data[n:]\n\n# Loading data\ndef get_batch(split):\n    \"\"\"Generate batch of data input x and target y.\"\"\"\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    return x.to(device), y.to(device)\n\n# Loss estimating\n@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    m.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_interval)\n        for k in range(eval_interval):\n            X, Y = get_batch(split) \n            logits, loss = m(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    m.train()\n    return out\n\n# Attention\nclass Head(nn.Module):\n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(n_embed, head_size, bias=False)\n        self.query = nn.Linear(n_embed, head_size, bias=False)\n        self.value = nn.Linear(n_embed, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n\n    def forward(self, x):\n        B, T, C = x.shape\n        Q, K, V = self.query(x), self.key(x), self.value(x)\n        scaled_score = Q @ K.transpose(-2,-1) * (C**-0.5)\n        masked_scaled_score = scaled_score.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n        attention = F.softmax(masked_scaled_score, dim=-1)\n        out = attention @ V\n        return out\n\n# MUlti Head attension\nclass MultiHeadAttension(nn.Module):\n    \"\"\"multiple heads of self attension.\"\"\"\n\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n\n    def forward(self, x):\n        return torch.cat([h(x) for h in self.heads], dim=-1)\n    \n\n# Model\nclass BigramLanguageModel(nn.Module):\n    def __init__(self, vocab_size):\n        super().__init__()\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n        self.self_att_head = MultiHeadAttension(4, n_embed//4) # Added attension here (number of heads = 4, attension head size = 8)\n        self.lm_head = nn.Linear(n_embed, vocab_size)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n        token_embed = self.token_embedding_table(idx)\n        positions = torch.arange(T, device=device)\n        position_embed = self.position_embedding_table(positions)\n        x = token_embed + position_embed\n        attn_out = self.self_att_head(x)\n        logits = self.lm_head(attn_out)\n        \n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        for _ in range(max_new_tokens):\n            idx_cond = idx[:, -block_size:]\n            logits, loss = self(idx_cond)\n            logits = logits[:, -1, :]\n            probs = F.softmax(logits, dim=-1)\n            idx_next = torch.multinomial(probs, num_samples=1)\n            idx = torch.cat((idx, idx_next), dim=1)\n        return idx\n\n# Initialize model and move to device\nm = BigramLanguageModel(vocab_size).to(device) \noptim = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n\n# Training loop\nfor steps in range(max_iters):\n    if steps % 10000 == 0:\n        losses = estimate_loss()\n        print(f\"step {steps}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n    \n    xb, yb = get_batch('train')\n    logits, loss = m(xb, yb)\n    optim.zero_grad(set_to_none=True)\n    loss.backward()\n    optim.step()\n\n# Final loss\nlosses = estimate_loss()\nprint(f\"Final: step {steps}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n\n# Generation\nidx = torch.zeros((1, 1), dtype=torch.long, device=device)  \nprint(decode(m.generate(idx, max_new_tokens=1000)[0].tolist()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T10:01:03.663490Z","iopub.execute_input":"2025-07-16T10:01:03.663748Z","iopub.status.idle":"2025-07-16T10:12:31.951555Z","shell.execute_reply.started":"2025-07-16T10:01:03.663728Z","shell.execute_reply":"2025-07-16T10:12:31.950825Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nstep 0: train loss 4.1734, val loss 4.1755\nstep 10000: train loss 2.0850, val loss 2.1583\nstep 20000: train loss 2.0343, val loss 2.1411\nstep 30000: train loss 2.0114, val loss 2.1313\nstep 40000: train loss 2.0027, val loss 2.1234\nstep 50000: train loss 1.9941, val loss 2.1190\nstep 60000: train loss 1.9835, val loss 2.1077\nstep 70000: train loss 1.9791, val loss 2.1052\nstep 80000: train loss 1.9744, val loss 2.1075\nstep 90000: train loss 1.9697, val loss 2.1032\nFinal: step 99999: train loss 1.9655, val loss 2.0919\n\nWill befork\nThowe, This by bloow: sere obe to: anthruand me?\nI ands:\nWhith fould worvet?\nF AULADY, endway, my fef's a wizonour\nYou, froof is heirth it nown that miree sent, wilatisntiliove to and the nown is wans!\n\nAUFIDIA:\nSher-hus, oncevey: thy havess, why mold nou normopetelives;\nPromy wod mot ake oyou so what eight, the mosostill weeshime st so mowe fore\nTo king thrufirfor are\nTist my monge maled\nAnd, af Pried my of.\n\nHENRY BOLK:\nSby am\nSads.\nWe it gent, huin cour ayrandy If his chan you!\nMy time not bee grith for ong my mesone and sevirent, by wer'ed.\n\nCORIOLANUS:\nWhe wilt wery ome.\n\nThuch frepy.\nWhith migh.\n\nAndiast ould imank.\n\nLUCIOPTINGS:\nTo to imapass\nLeploss.\nMy vit sties: ther his re ever as nyrect Clongs\nTunkist, on duche nee, ome\nFor therbeakes agave ving wead kno with self For do so llold?\n\nGivy, mor advisurs?\nTheink, thour noth, four?\nFy\nThous, to this ofter nown ce sut nexip!\nAnd, wwhat thavitth ouslep, Bliellk thou so hat ther bece watch time wout ighe an;\nTo knot our\n","output_type":"stream"}],"execution_count":50},{"cell_type":"markdown","source":"We have get some better answer.","metadata":{}},{"cell_type":"markdown","source":"## Feed forward network","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\n# Set seed\ntorch.manual_seed(1337)\n\n# Hyperparameters\nblock_size = 32\nbatch_size = 64\nmax_iters = 100_000\neval_interval = 1000\nlearning_rate = 1e-3\nn_embed = 32\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"Using device: {device}\")\n\n# Dataset\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\n# Preprocess\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\n\n# Encode Decode\nstring_to_int = { ch:i for i,ch in enumerate(chars) }\nint_to_string = { i:ch for i,ch in enumerate(chars) }\nencode = lambda s: [string_to_int[c] for c in s]\ndecode = lambda l: ''.join([int_to_string[i] for i in l])\n\n# Split data\ndata = torch.tensor(encode(text), dtype=torch.long)\nn = int(0.9*len(data))\ntrain_data = data[:n]\nval_data = data[n:]\n\n# Loading data\ndef get_batch(split):\n    \"\"\"Generate batch of data input x and target y.\"\"\"\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    return x.to(device), y.to(device)\n\n# Loss estimating\n@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    m.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_interval)\n        for k in range(eval_interval):\n            X, Y = get_batch(split) \n            logits, loss = m(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    m.train()\n    return out\n\n# Attention\nclass Head(nn.Module):\n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(n_embed, head_size, bias=False)\n        self.query = nn.Linear(n_embed, head_size, bias=False)\n        self.value = nn.Linear(n_embed, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n\n    def forward(self, x):\n        B, T, C = x.shape\n        Q, K, V = self.query(x), self.key(x), self.value(x)\n        scaled_score = Q @ K.transpose(-2,-1) * (C**-0.5)\n        masked_scaled_score = scaled_score.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n        attention = F.softmax(masked_scaled_score, dim=-1)\n        out = attention @ V\n        return out\n\n# MUlti Head attension\nclass MultiHeadAttension(nn.Module):\n    \"\"\"multiple heads of self attension.\"\"\"\n\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n\n    def forward(self, x):\n        return torch.cat([h(x) for h in self.heads], dim=-1)\n\n\n# FeedForward Network\nclass FeedForward(nn.Module):\n\n    def __init__(self, n_embed):\n        super().__init__()\n\n        self.net = nn.Sequential(\n            nn.Linear(n_embed, n_embed),\n            nn.ReLU(),\n            nn.Linear(n_embed, n_embed)\n            \n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n    \n\n# Model\nclass BigramLanguageModel(nn.Module):\n    def __init__(self, vocab_size):\n        super().__init__()\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n        self.self_att_head = MultiHeadAttension(4, n_embed//4) \n        self.ffn = FeedForward(n_embed) # Feed forward network.\n        self.lm_head = nn.Linear(n_embed, vocab_size)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n        token_embed = self.token_embedding_table(idx)\n        positions = torch.arange(T, device=device)\n        position_embed = self.position_embedding_table(positions)\n        x = token_embed + position_embed\n        attn_out = self.self_att_head(x)\n        ffn_attention_out = self.ffn(x) # Feed forward layer.\n        logits = self.lm_head(ffn_attention_out)\n        \n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        for _ in range(max_new_tokens):\n            idx_cond = idx[:, -block_size:]\n            logits, loss = self(idx_cond)\n            logits = logits[:, -1, :]\n            probs = F.softmax(logits, dim=-1)\n            idx_next = torch.multinomial(probs, num_samples=1)\n            idx = torch.cat((idx, idx_next), dim=1)\n        return idx\n\n# Initialize model and move to device\nm = BigramLanguageModel(vocab_size).to(device) \noptim = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n\n# Training loop\nfor steps in range(max_iters):\n    if steps % 10000 == 0:\n        losses = estimate_loss()\n        print(f\"step {steps}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n    \n    xb, yb = get_batch('train')\n    logits, loss = m(xb, yb)\n    optim.zero_grad(set_to_none=True)\n    loss.backward()\n    optim.step()\n\n# Final loss\nlosses = estimate_loss()\nprint(f\"Final: step {steps}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n\n# Generation\nidx = torch.zeros((1, 1), dtype=torch.long, device=device)  \nprint(decode(m.generate(idx, max_new_tokens=1000)[0].tolist()))","metadata":{"id":"AgHtGtf13do6","trusted":true,"execution":{"iopub.status.busy":"2025-07-16T12:57:45.377742Z","iopub.execute_input":"2025-07-16T12:57:45.378074Z","iopub.status.idle":"2025-07-16T13:06:28.596153Z","shell.execute_reply.started":"2025-07-16T12:57:45.378046Z","shell.execute_reply":"2025-07-16T13:06:28.595547Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nstep 0: train loss 4.2275, val loss 4.2292\nstep 10000: train loss 2.4600, val loss 2.4871\nstep 20000: train loss 2.4583, val loss 2.4908\nstep 30000: train loss 2.4567, val loss 2.4927\nstep 40000: train loss 2.4578, val loss 2.4873\nstep 50000: train loss 2.4577, val loss 2.4873\nstep 60000: train loss 2.4560, val loss 2.4903\nstep 70000: train loss 2.4580, val loss 2.4897\nstep 80000: train loss 2.4559, val loss 2.4901\nstep 90000: train loss 2.4557, val loss 2.4888\nFinal: step 99999: train loss 2.4567, val loss 2.4862\n\n\n\nCExthy brid owindakis by ble\n\nHAPen bube d e.\nS:\nO:\nIS:\nFalatanss:\nWanthar u qur, vet?\nF dilasoate awice my.\n\nHastarom oroup\nYowhthatof is h ble mil ndill, ath iree senghin lat Heridrovets, and Win nghir.\nThanousel lind me l.\nMAshe ce hiry:\nSupr aisspllw y.\nHerindu noroopetelaves\nMP:\n\nPl, d mothakllo Windo whth eisbyo wie m dourive we hidend t so mower; te\n\nAN ad nthrupt f s ar igr t m:\n\nThiny aleronth,\nMad\nWhed my o myoff-\nLIERor,\n\nby ak\nSadsal this ghesthidin cour ay aney Iry ts I fr y ce.\nJonghe nd, bemary.\nYof 'sour mend sora an hy t-senomeshthe men.\nWand thot sulin with llety od, wiourco ffepyotssththas l.\nTAn.\nMourethal wave.\nse ed Pe bene ovetour?\nCasscher os cok hedin tie s ind aus be fe f tas ny, ct Clo ghasundisthou ld, I fo, moxcharereanthataker aghercobun ws m k s withoumas Fond thacollo INour id, mersed\nFourd?\nTI idurd po venond, d Cad ty\nK:\nBIUSoou tiund thornofen e sutan wiporthare whanothavitthers, spe Bllollke, on s h O, t pan, ce wat d tive wout ir f au;\n\nFe cen oue\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## Adding Block","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\n# Set seed\ntorch.manual_seed(1337)\n\n# Hyperparameters\nblock_size = 32\nbatch_size = 64\nmax_iters = 100_000\neval_interval = 1000\nlearning_rate = 1e-3\nn_embed = 32\nn_head = 4\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"Using device: {device}\")\n\n# Dataset\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\n# Preprocess\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\n\n# Encode Decode\nstring_to_int = { ch:i for i,ch in enumerate(chars) }\nint_to_string = { i:ch for i,ch in enumerate(chars) }\nencode = lambda s: [string_to_int[c] for c in s]\ndecode = lambda l: ''.join([int_to_string[i] for i in l])\n\n# Split data\ndata = torch.tensor(encode(text), dtype=torch.long)\nn = int(0.9*len(data))\ntrain_data = data[:n]\nval_data = data[n:]\n\n# Loading data\ndef get_batch(split):\n    \"\"\"Generate batch of data input x and target y.\"\"\"\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    return x.to(device), y.to(device)\n\n# Loss estimating\n@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    m.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_interval)\n        for k in range(eval_interval):\n            X, Y = get_batch(split) \n            logits, loss = m(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    m.train()\n    return out\n\n# Attention\nclass Head(nn.Module):\n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(n_embed, head_size, bias=False)\n        self.query = nn.Linear(n_embed, head_size, bias=False)\n        self.value = nn.Linear(n_embed, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n\n    def forward(self, x):\n        B, T, C = x.shape\n        Q, K, V = self.query(x), self.key(x), self.value(x)\n        scaled_score = Q @ K.transpose(-2,-1) * (C**-0.5)\n        masked_scaled_score = scaled_score.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n        attention = F.softmax(masked_scaled_score, dim=-1)\n        out = attention @ V\n        return out\n\n# MUlti Head attension\nclass MultiHeadAttension(nn.Module):\n    \"\"\"multiple heads of self attension.\"\"\"\n\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n\n    def forward(self, x):\n        return torch.cat([h(x) for h in self.heads], dim=-1)\n\n\n# FeedForward Network\nclass FeedForward(nn.Module):\n\n    def __init__(self, n_embed):\n        super().__init__()\n\n        self.net = nn.Sequential(\n            nn.Linear(n_embed, n_embed),\n            nn.ReLU()\n            \n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n    \nclass Block(nn.Module):\n    \"\"\"Transformer wrap block: communication followed by computation.\"\"\"\n\n    def __init__(self, n_embed, n_head):\n        super().__init__()\n        head_size = n_embed // n_head\n        self.self_att_head = MultiHeadAttension(4, n_embed//4) \n        self.ffn = FeedForward(n_embed) # Feed forward network.\n\n    def forward(self, x):\n        x = self.self_att_head(x)\n        x = self.ffn(x)\n        return x\n        \n# Model\nclass BigramLanguageModel(nn.Module):\n    def __init__(self, vocab_size):\n        super().__init__()\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n        self.block = nn.Sequential(\n            Block(n_embed, n_head),\n            Block(n_embed, n_head),\n            Block(n_embed, n_head),\n        )\n        \n        self.lm_head = nn.Linear(n_embed, vocab_size)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n        token_embed = self.token_embedding_table(idx)\n        positions = torch.arange(T, device=device)\n        position_embed = self.position_embedding_table(positions)\n        x = token_embed + position_embed\n        x = self.block(x)\n        logits = self.lm_head(x)\n        \n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        for _ in range(max_new_tokens):\n            idx_cond = idx[:, -block_size:]\n            logits, loss = self(idx_cond)\n            logits = logits[:, -1, :]\n            probs = F.softmax(logits, dim=-1)\n            idx_next = torch.multinomial(probs, num_samples=1)\n            idx = torch.cat((idx, idx_next), dim=1)\n        return idx\n\n# Initialize model and move to device\nm = BigramLanguageModel(vocab_size).to(device) \noptim = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n\n# Training loop\nfor steps in range(max_iters):\n    if steps % 10000 == 0:\n        losses = estimate_loss()\n        print(f\"step {steps}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n    \n    xb, yb = get_batch('train')\n    logits, loss = m(xb, yb)\n    optim.zero_grad(set_to_none=True)\n    loss.backward()\n    optim.step()\n\n# Final loss\nlosses = estimate_loss()\nprint(f\"Final: step {steps}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n\n# Generation\nidx = torch.zeros((1, 1), dtype=torch.long, device=device)  \nprint(decode(m.generate(idx, max_new_tokens=1000)[0].tolist()))","metadata":{"id":"DpO2zC7t3drl","trusted":true,"execution":{"iopub.status.busy":"2025-07-16T13:11:29.255603Z","iopub.execute_input":"2025-07-16T13:11:29.256194Z","iopub.status.idle":"2025-07-16T13:35:18.521098Z","shell.execute_reply.started":"2025-07-16T13:11:29.256172Z","shell.execute_reply":"2025-07-16T13:35:18.520359Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nstep 0: train loss 4.1952, val loss 4.1954\nstep 10000: train loss 2.1857, val loss 2.2270\nstep 20000: train loss 2.0760, val loss 2.1556\nstep 30000: train loss 2.0137, val loss 2.1195\nstep 40000: train loss 1.9777, val loss 2.0915\nstep 50000: train loss 1.9350, val loss 2.0602\nstep 60000: train loss 1.9069, val loss 2.0452\nstep 70000: train loss 1.8862, val loss 2.0501\nstep 80000: train loss 1.8677, val loss 2.0271\nstep 90000: train loss 1.8544, val loss 2.0227\nFinal: step 99999: train loss 1.8501, val loss 2.0225\n\n\nYre befork\nThow and is by be madient buay dandanth them me?\nLeands:\nWhither us hath but ceather ane awched you,\nThan of onoughtow, fromf in heart mil;\nI lit,\nWhile eus, I in latient drov the do quin nown is wace!\nAl lind me littius, on him so up;\nOr spelw yem love us you peterat gonountly, demether\nTo Willond do eiighs to them, and Then him us poors\nAs for unte and thrupter so;\nAngies muf's to male of, to fier of my of.\n\nHENRLA:\nBreight as\nardaal actious sto upon mustear tey it-pust,\nAnd for his no to ok beary.\nYou 'Worranths so a ce his me?\n\nTould extentreand thou agains wiltly to on,\nThou cour some should sou.\n\nAEEn sountly injestsse evere benemoved: if passe to ousigk over boie so upon sixe fe'd tas noth'd Clongs\nThen fum no duch fore, ougness the beare bagaince's\nAs am soul vooks long of the welk not a spandcess I subjil I inked poiven'd.\n\nGo thied\nThough oo deius the shown. eys to aliportanot what than to couses Enclise kears trow an the it.\nThat I, time wout in for Gly eight our\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"## Optimizations","metadata":{}},{"cell_type":"markdown","source":"### Residual connection ","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\n# Set seed\ntorch.manual_seed(1337)\n\n# Hyperparameters\nblock_size = 32\nbatch_size = 64\nmax_iters = 100_000\neval_interval = 1000\nlearning_rate = 1e-3\nn_embed = 32\nn_head = 4\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"Using device: {device}\")\n\n# Dataset\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\n# Preprocess\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\n\n# Encode Decode\nstring_to_int = { ch:i for i,ch in enumerate(chars) }\nint_to_string = { i:ch for i,ch in enumerate(chars) }\nencode = lambda s: [string_to_int[c] for c in s]\ndecode = lambda l: ''.join([int_to_string[i] for i in l])\n\n# Split data\ndata = torch.tensor(encode(text), dtype=torch.long)\nn = int(0.9*len(data))\ntrain_data = data[:n]\nval_data = data[n:]\n\n# Loading data\ndef get_batch(split):\n    \"\"\"Generate batch of data input x and target y.\"\"\"\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    return x.to(device), y.to(device)\n\n# Loss estimating\n@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    m.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_interval)\n        for k in range(eval_interval):\n            X, Y = get_batch(split) \n            logits, loss = m(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    m.train()\n    return out\n\n# Attention\nclass Head(nn.Module):\n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(n_embed, head_size, bias=False)\n        self.query = nn.Linear(n_embed, head_size, bias=False)\n        self.value = nn.Linear(n_embed, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n\n    def forward(self, x):\n        B, T, C = x.shape\n        Q, K, V = self.query(x), self.key(x), self.value(x)\n        scaled_score = Q @ K.transpose(-2,-1) * (C**-0.5)\n        masked_scaled_score = scaled_score.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n        attention = F.softmax(masked_scaled_score, dim=-1)\n        out = attention @ V\n        return out\n\n# MUlti Head attension\nclass MultiHeadAttension(nn.Module):\n    \"\"\"multiple heads of self attension.\"\"\"\n\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        self.projection = nn.Linear(num_heads * head_size, n_embed) # For residual conn.\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.projection(out)\n        return out\n\n\n# FeedForward Network\nclass FeedForward(nn.Module):\n\n    def __init__(self, n_embed):\n        super().__init__()\n\n        self.net = nn.Sequential(\n            nn.Linear(n_embed, 4*n_embed), # 4 times to complete 128 as described in paper.\n            nn.ReLU(),\n            nn.Linear(4*n_embed, n_embed),\n            \n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n    \nclass Block(nn.Module):\n    \"\"\"Transformer wrap block: communication followed by computation.\"\"\"\n\n    def __init__(self, n_embed, n_head):\n        super().__init__()\n        head_size = n_embed // n_head\n        self.self_att_head = MultiHeadAttension(4, n_embed//4) \n        self.ffn = FeedForward(n_embed) # Feed forward network.\n\n    def forward(self, x):\n        x = x + self.self_att_head(x) # Added x for residual conn.\n        x = x + self.ffn(x) # Added x for residual conn.\n        return x\n        \n# Model\nclass BigramLanguageModel(nn.Module):\n    def __init__(self, vocab_size):\n        super().__init__()\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n        self.block = nn.Sequential(\n            Block(n_embed, n_head),\n            Block(n_embed, n_head),\n            Block(n_embed, n_head),\n        )\n        \n        self.lm_head = nn.Linear(n_embed, vocab_size)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n        token_embed = self.token_embedding_table(idx)\n        positions = torch.arange(T, device=device)\n        position_embed = self.position_embedding_table(positions)\n        x = token_embed + position_embed\n        x = self.block(x)\n        logits = self.lm_head(x)\n        \n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        for _ in range(max_new_tokens):\n            idx_cond = idx[:, -block_size:]\n            logits, loss = self(idx_cond)\n            logits = logits[:, -1, :]\n            probs = F.softmax(logits, dim=-1)\n            idx_next = torch.multinomial(probs, num_samples=1)\n            idx = torch.cat((idx, idx_next), dim=1)\n        return idx\n\n# Initialize model and move to device\nm = BigramLanguageModel(vocab_size).to(device) \noptim = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n\n# Training loop\nfor steps in range(max_iters):\n    if steps % 10000 == 0:\n        losses = estimate_loss()\n        print(f\"step {steps}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n    \n    xb, yb = get_batch('train')\n    logits, loss = m(xb, yb)\n    optim.zero_grad(set_to_none=True)\n    loss.backward()\n    optim.step()\n\n# Final loss\nlosses = estimate_loss()\nprint(f\"Final: step {steps}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n\n# Generation\nidx = torch.zeros((1, 1), dtype=torch.long, device=device)  \nprint(decode(m.generate(idx, max_new_tokens=1000)[0].tolist()))","metadata":{"id":"PTt0l1JC3duX","trusted":true,"execution":{"iopub.status.busy":"2025-07-16T13:38:42.991821Z","iopub.execute_input":"2025-07-16T13:38:42.992082Z","iopub.status.idle":"2025-07-16T14:04:49.357445Z","shell.execute_reply.started":"2025-07-16T13:38:42.992064Z","shell.execute_reply":"2025-07-16T14:04:49.356758Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nstep 0: train loss 4.6987, val loss 4.7088\nstep 10000: train loss 1.6954, val loss 1.8743\nstep 20000: train loss 1.6297, val loss 1.8283\nstep 30000: train loss 1.6006, val loss 1.8045\nstep 40000: train loss 1.5874, val loss 1.7966\nstep 50000: train loss 1.5692, val loss 1.7839\nstep 60000: train loss 1.5642, val loss 1.7834\nstep 70000: train loss 1.5514, val loss 1.7950\nstep 80000: train loss 1.5475, val loss 1.7601\nstep 90000: train loss 1.5385, val loss 1.7708\nFinal: step 99999: train loss 1.5327, val loss 1.7631\n\n\nKING RICHARD II:\nShall be be made to beauty enamage-dam the change:\nWhither us hath bed redils\nHath away, my fears, to zost he own his fits her!\nAlaw nowle, at misters, I in latest in oveys, and the next is wail!\n\nAUFIDIUS:\nThe husbalt him speak; and the tyrantly,\nHe mayor this lown'd\nthe o' mother\nTo Willong do evil's worth dostical gentleman poor of his guard and thrupt for treaging me to thy fledgath,\nMadam, death? I am I sup!\n\nGREGORY:\nSads.\nWith Edward hoight to maranny it?\n\nMONTAGUEES:\nMy fire you see guilty wing gallam so a change.\n\nDUKE OF YORK:\nArtand thou speak so not us body.\n\nTYBR:\nIf omanst at might. Ay, as I thall jest:\nYour Peace, moved: if Catardion:\nAnd move, being: in doubt, a gone as nyreat' loves\nThe heart, if I condemn more.\n\nNORTHUMBERLAND:\nThis cowar soul. Do yeld monder so lady.\n\nPrivy, make himself?\nTheir he poison'd more thief is this of thee\nThe rooden eyes more pooth ours-beding it, bestest,\nBecome made thou knew pain.\n\nBENVOLIO:\nYou there for Gloucesterous\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"## Layer Normalization","metadata":{}},{"cell_type":"markdown","source":"Layer normalization normalizes the activations of a layer for each training example, so that they have mean 0 and variance 1.","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\n# Set seed\ntorch.manual_seed(1337)\n\n# Hyperparameters\nblock_size = 32\nbatch_size = 64\nmax_iters = 100_000\neval_interval = 1000\nlearning_rate = 1e-3\nn_embed = 32\nn_head = 4\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"Using device: {device}\")\n\n# Dataset\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\n# Preprocess\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\n\n# Encode Decode\nstring_to_int = { ch:i for i,ch in enumerate(chars) }\nint_to_string = { i:ch for i,ch in enumerate(chars) }\nencode = lambda s: [string_to_int[c] for c in s]\ndecode = lambda l: ''.join([int_to_string[i] for i in l])\n\n# Split data\ndata = torch.tensor(encode(text), dtype=torch.long)\nn = int(0.9*len(data))\ntrain_data = data[:n]\nval_data = data[n:]\n\n# Loading data\ndef get_batch(split):\n    \"\"\"Generate batch of data input x and target y.\"\"\"\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    return x.to(device), y.to(device)\n\n# Loss estimating\n@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    m.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_interval)\n        for k in range(eval_interval):\n            X, Y = get_batch(split) \n            logits, loss = m(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    m.train()\n    return out\n\n# Attention\nclass Head(nn.Module):\n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(n_embed, head_size, bias=False)\n        self.query = nn.Linear(n_embed, head_size, bias=False)\n        self.value = nn.Linear(n_embed, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n\n    def forward(self, x):\n        B, T, C = x.shape\n        Q, K, V = self.query(x), self.key(x), self.value(x)\n        scaled_score = Q @ K.transpose(-2,-1) * (C**-0.5)\n        masked_scaled_score = scaled_score.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n        attention = F.softmax(masked_scaled_score, dim=-1)\n        out = attention @ V\n        return out\n\n# MUlti Head attension\nclass MultiHeadAttension(nn.Module):\n    \"\"\"multiple heads of self attension.\"\"\"\n\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        self.projection = nn.Linear(num_heads * head_size, n_embed) # For residual conn.\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.projection(out)\n        return out\n\n\n# FeedForward Network\nclass FeedForward(nn.Module):\n\n    def __init__(self, n_embed):\n        super().__init__()\n\n        self.net = nn.Sequential(\n            nn.Linear(n_embed, 4*n_embed), # 4 times to complete 128 as described in paper.\n            nn.ReLU(),\n            nn.Linear(4*n_embed, n_embed),\n            \n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n    \nclass Block(nn.Module):\n    \"\"\"Transformer wrap block: communication followed by computation.\"\"\"\n\n    def __init__(self, n_embed, n_head):\n        super().__init__()\n        head_size = n_embed // n_head\n        self.self_att_head = MultiHeadAttension(4, n_embed//4) \n        self.ffn = FeedForward(n_embed) # Feed forward network.\n        self.ln1 = nn.LayerNorm(n_embed)\n        self.ln2 = nn.LayerNorm(n_embed)\n        \n\n    def forward(self, x):\n        x = x + self.self_att_head(self.ln1(x)) # Added x for residual conn.\n        x = x + self.ffn(self.ln2(x)) # Added x for residual conn.\n        return x\n        \n# Model\nclass BigramLanguageModel(nn.Module):\n    def __init__(self, vocab_size):\n        super().__init__()\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n        self.block = nn.Sequential(\n            Block(n_embed, n_head),\n            Block(n_embed, n_head),\n            Block(n_embed, n_head),\n            nn.LayerNorm(n_embed)\n        )\n        \n        self.lm_head = nn.Linear(n_embed, vocab_size)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n        token_embed = self.token_embedding_table(idx)\n        positions = torch.arange(T, device=device)\n        position_embed = self.position_embedding_table(positions)\n        x = token_embed + position_embed\n        x = self.block(x)\n        logits = self.lm_head(x)\n        \n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        for _ in range(max_new_tokens):\n            idx_cond = idx[:, -block_size:]\n            logits, loss = self(idx_cond)\n            logits = logits[:, -1, :]\n            probs = F.softmax(logits, dim=-1)\n            idx_next = torch.multinomial(probs, num_samples=1)\n            idx = torch.cat((idx, idx_next), dim=1)\n        return idx\n\n# Initialize model and move to device\nm = BigramLanguageModel(vocab_size).to(device) \noptim = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n\n# Training loop\nfor steps in range(max_iters):\n    if steps % 10000 == 0:\n        losses = estimate_loss()\n        print(f\"step {steps}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n    \n    xb, yb = get_batch('train')\n    logits, loss = m(xb, yb)\n    optim.zero_grad(set_to_none=True)\n    loss.backward()\n    optim.step()\n\n# Final loss\nlosses = estimate_loss()\nprint(f\"Final: step {steps}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n\n# Generation\nidx = torch.zeros((1, 1), dtype=torch.long, device=device)  \nprint(decode(m.generate(idx, max_new_tokens=1000)[0].tolist()))","metadata":{"id":"QLsVpZnZ3dws","trusted":true,"execution":{"iopub.status.busy":"2025-07-16T14:33:20.729064Z","iopub.execute_input":"2025-07-16T14:33:20.729324Z","iopub.status.idle":"2025-07-16T15:00:55.124672Z","shell.execute_reply.started":"2025-07-16T14:33:20.729305Z","shell.execute_reply":"2025-07-16T15:00:55.123857Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nstep 0: train loss 4.4188, val loss 4.4242\nstep 10000: train loss 1.6934, val loss 1.8542\nstep 20000: train loss 1.6375, val loss 1.8262\nstep 30000: train loss 1.6075, val loss 1.7871\nstep 40000: train loss 1.5902, val loss 1.7902\nstep 50000: train loss 1.5752, val loss 1.7734\nstep 60000: train loss 1.5690, val loss 1.7710\nstep 70000: train loss 1.5594, val loss 1.7622\nstep 80000: train loss 1.5568, val loss 1.7500\nstep 90000: train loss 1.5447, val loss 1.7464\nFinal: step 99999: train loss 1.5357, val loss 1.7357\n\n\nAll thy bridl'd and is so blood: the bobbedon.\nStirtake me?\nThat shall this usque her backeth!\nHath away, my facts a rezans heavens, to that I could well\nWho even well, I in latestly rove thee.\n\nWARTIS:\nWill justs liking tear.\nMaster now, you are aiss hew you love'd nor\nTo this destroy like to that\nmoal-now what evingle, and forth, Then, in he poor give; thereford none firs so;\nAnd he must with allow; which Prind my offord's subjestice is:\nSadsal the Edward him best asaranty, repulectandance.\n\nMONTES:\nPose, good she's his thank had; myself postent!\nNot throating stars in son't:\nStill be gone: if some should souble\nIn as fortune jourabe even'd helply.\nLived asscator's counsel; boys stings his read, lady nyrb,\nMake my sun?\nWhat, low I do, so more.\n\nNORTHUMBERLAND:\nThink'd all TI with self From the welcomes a vy,\nThesed is, he desile\nby the none, do thither, thou of timed.\n\nDUCHESS OF YORK:\nSportan'd what those the silst, Blies kings that knew you, drewat doth York?\n\nShephance on her our\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"## Model Scaling","metadata":{}},{"cell_type":"markdown","source":"- Introduce layers of blocks\n- Add dropout in FeedForward\n- Add dropout in MultiHead\n- Add dropout in Head\n- Change batch size\n- Change block size\n- chenge embedding size\n- Change number of heads","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\n# Set seed\ntorch.manual_seed(1337)\n\n# Hyperparameters\nblock_size = 64\nbatch_size = 256\nmax_iters = 100_000\neval_interval = 1000\nlearning_rate = 1e-4\nn_embed = 384\nn_head = 6\nn_layer = 6 # Number of block,\ndropout = 0.2\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"Using device: {device}\")\n\n# Dataset\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\n# Preprocess\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\n\n# Encode Decode\nstring_to_int = { ch:i for i,ch in enumerate(chars) }\nint_to_string = { i:ch for i,ch in enumerate(chars) }\nencode = lambda s: [string_to_int[c] for c in s]\ndecode = lambda l: ''.join([int_to_string[i] for i in l])\n\n# Split data\ndata = torch.tensor(encode(text), dtype=torch.long)\nn = int(0.9*len(data))\ntrain_data = data[:n]\nval_data = data[n:]\n\n# Loading data\ndef get_batch(split):\n    \"\"\"Generate batch of data input x and target y.\"\"\"\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    return x.to(device), y.to(device)\n\n# Loss estimating\n@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    m.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_interval)\n        for k in range(eval_interval):\n            X, Y = get_batch(split) \n            logits, loss = m(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    m.train()\n    return out\n\n# Attention\nclass Head(nn.Module):\n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(n_embed, head_size, bias=False)\n        self.query = nn.Linear(n_embed, head_size, bias=False)\n        self.value = nn.Linear(n_embed, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        B, T, C = x.shape\n        Q, K, V = self.query(x), self.key(x), self.value(x)\n        scaled_score = Q @ K.transpose(-2,-1) * (C**-0.5)\n        masked_scaled_score = scaled_score.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n        attention = F.softmax(masked_scaled_score, dim=-1)\n        droped_attension = self.dropout(attention)\n        out = droped_attension @ V\n        return out\n\n# MUlti Head attension\nclass MultiHeadAttension(nn.Module):\n    \"\"\"multiple heads of self attension.\"\"\"\n\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        self.projection = nn.Linear(num_heads * head_size, n_embed) # For residual conn.\n        self.dropout =nn.Dropout(dropout)\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.dropout(self.projection(out))\n        return out\n\n\n# FeedForward Network\nclass FeedForward(nn.Module):\n\n    def __init__(self, n_embed):\n        super().__init__()\n\n        self.net = nn.Sequential(\n            nn.Linear(n_embed, 4*n_embed), # 4 times to complete 128 as described in paper.\n            nn.ReLU(),\n            nn.Linear(4*n_embed, n_embed),\n            nn.Dropout(dropout)\n            \n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n    \nclass Block(nn.Module):\n    \"\"\"Transformer wrap block: communication followed by computation.\"\"\"\n\n    def __init__(self, n_embed, n_head):\n        super().__init__()\n        head_size = n_embed // n_head\n        self.self_att_head = MultiHeadAttension(4, n_embed//4) \n        self.ffn = FeedForward(n_embed) # Feed forward network.\n        self.ln1 = nn.LayerNorm(n_embed)\n        self.ln2 = nn.LayerNorm(n_embed)\n        \n\n    def forward(self, x):\n        x = x + self.self_att_head(self.ln1(x)) # Added x for residual conn.\n        x = x + self.ffn(self.ln2(x)) # Added x for residual conn.\n        return x\n        \n# Model\nclass BigramLanguageModel(nn.Module):\n    def __init__(self, vocab_size):\n        super().__init__()\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n        self.block = nn.Sequential(\n            *[Block(n_embed, n_head=n_head) for _ in range(n_layer)]\n        )\n\n        self.layernorm = nn.LayerNorm(n_embed)\n        \n        self.lm_head = nn.Linear(n_embed, vocab_size)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n        token_embed = self.token_embedding_table(idx)\n        positions = torch.arange(T, device=device)\n        position_embed = self.position_embedding_table(positions)\n        x = token_embed + position_embed\n        x = self.block(x)\n        x = self.layernorm(x)\n        logits = self.lm_head(x)\n        \n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        for _ in range(max_new_tokens):\n            idx_cond = idx[:, -block_size:]\n            logits, loss = self(idx_cond)\n            logits = logits[:, -1, :]\n            probs = F.softmax(logits, dim=-1)\n            idx_next = torch.multinomial(probs, num_samples=1)\n            idx = torch.cat((idx, idx_next), dim=1)\n        return idx\n\n# Initialize model and move to device\nm = BigramLanguageModel(vocab_size).to(device) \noptim = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n\n# Training loop\nfor steps in range(max_iters):\n    if steps % 10000 == 0:\n        losses = estimate_loss()\n        print(f\"step {steps}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n    \n    xb, yb = get_batch('train')\n    logits, loss = m(xb, yb)\n    optim.zero_grad(set_to_none=True)\n    loss.backward()\n    optim.step()\n\n# Final loss\nlosses = estimate_loss()\nprint(f\"Final: step {steps}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n\n# Generation\nidx = torch.zeros((1, 1), dtype=torch.long, device=device)  \nprint(decode(m.generate(idx, max_new_tokens=1000)[0].tolist()))","metadata":{"id":"OigeRY6U3dy8","trusted":true,"execution":{"iopub.status.busy":"2025-07-16T15:14:55.781133Z","iopub.execute_input":"2025-07-16T15:14:55.781393Z","iopub.status.idle":"2025-07-16T21:42:35.895728Z","shell.execute_reply.started":"2025-07-16T15:14:55.781376Z","shell.execute_reply":"2025-07-16T21:42:35.894903Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nstep 0: train loss 4.2752, val loss 4.2859\nstep 10000: train loss 1.1330, val loss 1.5142\nstep 20000: train loss 0.9079, val loss 1.6013\nstep 30000: train loss 0.7214, val loss 1.7495\nstep 40000: train loss 0.5761, val loss 1.9018\nstep 50000: train loss 0.4730, val loss 2.0438\nstep 60000: train loss 0.4023, val loss 2.1578\nstep 70000: train loss 0.3551, val loss 2.2541\nstep 80000: train loss 0.3206, val loss 2.3556\nstep 90000: train loss 0.2969, val loss 2.4451\nFinal: step 99999: train loss 0.2805, val loss 2.5099\n\nO, give me worse, for time of bloody times!\nAll drophere should I raint my state friends,\nI cratch'd the golden crown be mine;\nWith injuries thee can as this deep do it;\nAnd say you will, sweat to my curse and swells\nUpon what ever gave what men received\nYou shout a prince bow 'gainst Thursday me.\nThese have you done, and circly now,\nWhat spares the switch is that when I seem she learn\nThe peace of this adversary.\nHow now, Catizen:\nDo you think of her; served his majesty hath\nhad nothing else to amplent in this truly bow'!\n\nKING RICHARD II:\nDay, stand to your correct would, without the guilty further\nThe deep of bits and tyrannication\nPies: the king hath he depended;\nThe contents that happy gentle Peace.\n\nPOLIXENES:\nOne whom he loves all hide own no more\nBut instantly can ender you: the sword\nThen you play will serve; that willst seem to whip\nTo see her so made a peace of into your hands?\n\nNORTHUMBERLAND:\nWars you then become my king, if we stoop\nHer from the type gentle to complaint;\n\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"After **~10k** steps: Train loss continues to improve, but val loss starts getting worse the model is now starting to memorize the training data instead of generalizing which is **overfitting**. This took several hours in keggle P100 GPU to run thats why it would be hard to experiment and make it better.\n\nBut model scaling made the output significantly reasonable as compared to before scaling.","metadata":{}}]}